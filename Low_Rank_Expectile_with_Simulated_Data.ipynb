{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBpujo7qX1TWMelU9jT8/m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haytham918/low-rank-expectile/blob/main/Low_Rank_Expectile_with_Simulated_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wK4GRML4fmDp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib as plot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "\n",
        "np.random.seed(500)\n",
        "\n",
        "n_users = 50\n",
        "n_items = 40\n",
        "\n",
        "latent_dim = 2\n",
        "\n",
        "true_r = norm.rvs(size=n_users)\n",
        "true_c = norm.rvs(size=n_items)\n",
        "\n",
        "true_u = norm.rvs(size=(n_users, latent_dim))\n",
        "true_v = norm.rvs(size=(n_items, latent_dim))\n",
        "\n",
        "sigma = 0.5\n",
        "\n",
        "mu_X = np.outer(true_r, true_c) + np.dot(true_u, true_v.T)\n",
        "\n",
        "error = sigma * norm.rvs(size=(n_users, n_items))\n",
        "\n",
        "feature_matrix = mu_X + error\n",
        "\n",
        "prob_miss = 0.2\n",
        "\n",
        "missing = np.random.choice([True, False], size=(n_users, n_items), p=[prob_miss, 1- prob_miss])\n",
        "feature_matrix[missing] = np.nan\n",
        "\n",
        "nan_matrix = np.isnan(feature_matrix)\n",
        "print(feature_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38GC_c4_fx6d",
        "outputId": "676e6d71-b626-4ed5-e375-5b0702b9c0ab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[        nan  0.45365492 -1.14393308 ...         nan  0.10629064\n",
            "   0.73474957]\n",
            " [-0.40606608 -0.41561065  1.28503233 ... -2.09206259 -0.17398039\n",
            "  -1.52876877]\n",
            " [ 0.47972853  0.24906682  0.26750481 ... -1.43583817         nan\n",
            "  -0.62707475]\n",
            " ...\n",
            " [-1.22378181  0.05043373         nan ...  0.98055671  0.94802885\n",
            "  -0.38990545]\n",
            " [ 1.08027337         nan -0.36384272 ...  2.28791502 -4.96483045\n",
            "   1.11112067]\n",
            " [ 0.15508994 -0.21717586         nan ...  0.75743742  0.71706363\n",
            "   0.12058302]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into trainig/validation and exclude missing values\n",
        "train_data, val_data, train_mask, val_mask = train_test_split(feature_matrix, nan_matrix, test_size=0.2, random_state=445)\n",
        "\n",
        "\n",
        "# Create Tensors based on train/val data\n",
        "train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
        "print(train_tensor)\n",
        "print(train_mask)\n",
        "val_tensor = torch.tensor(val_data, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Model definition\n",
        "class LRModel(nn.Module):\n",
        "  def __init__(self, number_users, number_times, rank):\n",
        "    super().__init__()\n",
        "    self.user_factors = nn.Embedding(number_users, rank)\n",
        "    self.times_factors = nn.Embedding(number_times, rank)\n",
        "\n",
        "    self.user_bias = nn.Embedding(number_users, 1)\n",
        "    self.times_bias = nn.Embedding(number_times, 1)\n",
        "\n",
        "    # Initializing the bias terms to zeros\n",
        "    self.user_bias.weight.data.fill_(0.)\n",
        "    self.times_bias.weight.data.fill_(0.)\n",
        "\n",
        "\n",
        "  # Define forward propagation\n",
        "  def forward(self, user, times):\n",
        "    # print(self.user_factors(user).shape)\n",
        "    # print(self.times_factors(times).shape)\n",
        "    pred = self.user_factors(user) * self.times_factors(times)\n",
        "    pred = pred.sum(1, keepdim=False)\n",
        "    pred += self.user_bias(user).squeeze() + self.times_bias(times).squeeze()\n",
        "    return pred\n",
        "\n",
        "# Define Loss function excluding missing values\n",
        "def loss_func(predicted, actual, mask, tau):\n",
        "    # print(predicted.shape, actual.shape, mask.shape)\n",
        "    invert_mask = ~mask\n",
        "    # print(predicted[invert_mask].shape, actual[invert_mask].shape)\n",
        "\n",
        "    ### MSE LOSS\n",
        "    mseFUNC = nn.MSELoss();\n",
        "    return mseFUNC(predicted[invert_mask].view(-1), actual[invert_mask].view(-1))\n",
        "\n",
        "\n",
        "    loss = 0\n",
        "    invert_mask = invert_mask.reshape(-1)\n",
        "    count = 0\n",
        "    for i in range(predicted.shape[0]):\n",
        "      if(invert_mask[i]):\n",
        "        count += 1\n",
        "        difference = actual[i] - predicted[i]\n",
        "        loss += tau * difference ** 2 if difference >= 0 else (1-tau) * difference**2\n",
        "    # return loss(predicted[invert_mask].view(-1), actual[invert_mask].view(-1))\n",
        "    loss /= count\n",
        "    return loss\n",
        "\n",
        "# Define parameters in our case\n",
        "number_users, number_times = train_data.shape\n",
        "rank = 6\n",
        "model = LRModel(number_users, number_times, rank)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Epochs and CheckpointPath\n",
        "number_epochs = 300\n",
        "\n",
        "global_best_loss = float('inf')\n",
        "best_epoch = 0\n",
        "\n",
        "print(val_mask.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-aw-npnfzif",
        "outputId": "0107329e-e600-4eaf-8da9-6ec67129fca8"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4797,  0.2491,  0.2675,  ..., -1.4358,     nan, -0.6271],\n",
            "        [    nan,     nan,  0.7744,  ...,  0.7552, -2.4682,  1.2515],\n",
            "        [-0.2632,  1.3360,     nan,  ...,     nan, -2.4719, -2.0264],\n",
            "        ...,\n",
            "        [    nan,     nan,  0.5399,  ...,  0.7345, -0.0150,  1.0831],\n",
            "        [ 0.0783,  0.6225,     nan,  ...,  0.1428,  0.0469, -0.9662],\n",
            "        [ 1.1254, -0.1971, -0.9016,  ...,  1.4889,  0.2518,     nan]])\n",
            "[[False False False ... False  True False]\n",
            " [ True  True False ... False False False]\n",
            " [False False  True ...  True False False]\n",
            " ...\n",
            " [ True  True False ... False False False]\n",
            " [False False  True ... False False False]\n",
            " [False False False ... False False  True]]\n",
            "(10, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "for epoch in range(number_epochs):\n",
        "  user_indices = torch.arange(number_users).repeat_interleave(number_times)\n",
        "  time_indices = torch.arange(number_times).repeat(number_users)\n",
        "  output = model(user_indices, time_indices)\n",
        "\n",
        "  # Calculate loss\n",
        "  train_tensor_flat = train_tensor.view(-1)\n",
        "  train_mask_flat = train_mask.reshape(-1)\n",
        "  training_loss = loss_func(output, train_tensor_flat, train_mask_flat, 0.5)\n",
        "  if(epoch % 10 == 0):\n",
        "    print(f\"Epoch {epoch} loss is : \", training_loss)\n",
        "\n",
        "  # Backward\n",
        "  optimizer.zero_grad()\n",
        "  training_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    validation_num_user, validation_num_times = val_tensor.shape\n",
        "    validation_user_indices = torch.arange(validation_num_user).repeat_interleave(validation_num_times)\n",
        "    validation_time_indices = torch.arange(validation_num_times).repeat(validation_num_user)\n",
        "    validation_output = model(validation_user_indices, validation_time_indices)\n",
        "    validation_loss = loss_func(validation_output, val_tensor.view(-1), val_mask.reshape(-1), 0.5)\n",
        "    # print(\"Validation Loss\", validation_loss)\n",
        "\n",
        "  # print(f\"Epoch [{epoch + 1}/{number_epochs}]: Training Loss: {training_loss.item()}; Validation Loss: {validation_loss.item()}\")\n",
        "\n",
        "  if validation_loss < global_best_loss:\n",
        "      global_best_loss = validation_loss\n",
        "      best_epoch = epoch\n",
        "      # torch.save({\"Epoch\": epoch, \"Model_state_dict\": model.state_dict(), \"Optimizer_state_dict\": optimizer.state_dict(), \"Loss\": validation_loss},\n",
        "      #            f\"/content/drive/MyDrive/low-rank-expectile/checkpoints/model_checkpoint_epoch{epoch}.pt\")\n",
        "\n",
        "print(\"Best Validation Epoch: \", best_epoch + 1)\n",
        "print(\"Best Validation Loss: \", global_best_loss)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIRT79ZIh7U0",
        "outputId": "a5f29681-9b74-4a5f-dece-a7229c0825d1"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss is :  tensor(0.1300, grad_fn=<MseLossBackward0>)\n",
            "Epoch 10 loss is :  tensor(0.1300, grad_fn=<MseLossBackward0>)\n",
            "Epoch 20 loss is :  tensor(0.1300, grad_fn=<MseLossBackward0>)\n",
            "Epoch 30 loss is :  tensor(0.1299, grad_fn=<MseLossBackward0>)\n",
            "Epoch 40 loss is :  tensor(0.1299, grad_fn=<MseLossBackward0>)\n",
            "Epoch 50 loss is :  tensor(0.1299, grad_fn=<MseLossBackward0>)\n",
            "Epoch 60 loss is :  tensor(0.1299, grad_fn=<MseLossBackward0>)\n",
            "Epoch 70 loss is :  tensor(0.1299, grad_fn=<MseLossBackward0>)\n",
            "Epoch 80 loss is :  tensor(0.1298, grad_fn=<MseLossBackward0>)\n",
            "Epoch 90 loss is :  tensor(0.1298, grad_fn=<MseLossBackward0>)\n",
            "Epoch 100 loss is :  tensor(0.1298, grad_fn=<MseLossBackward0>)\n",
            "Epoch 110 loss is :  tensor(0.1298, grad_fn=<MseLossBackward0>)\n",
            "Epoch 120 loss is :  tensor(0.1298, grad_fn=<MseLossBackward0>)\n",
            "Epoch 130 loss is :  tensor(0.1297, grad_fn=<MseLossBackward0>)\n",
            "Epoch 140 loss is :  tensor(0.1297, grad_fn=<MseLossBackward0>)\n",
            "Epoch 150 loss is :  tensor(0.1297, grad_fn=<MseLossBackward0>)\n",
            "Epoch 160 loss is :  tensor(0.1297, grad_fn=<MseLossBackward0>)\n",
            "Epoch 170 loss is :  tensor(0.1297, grad_fn=<MseLossBackward0>)\n",
            "Epoch 180 loss is :  tensor(0.1296, grad_fn=<MseLossBackward0>)\n",
            "Epoch 190 loss is :  tensor(0.1296, grad_fn=<MseLossBackward0>)\n",
            "Epoch 200 loss is :  tensor(0.1296, grad_fn=<MseLossBackward0>)\n",
            "Epoch 210 loss is :  tensor(0.1296, grad_fn=<MseLossBackward0>)\n",
            "Epoch 220 loss is :  tensor(0.1296, grad_fn=<MseLossBackward0>)\n",
            "Epoch 230 loss is :  tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
            "Epoch 240 loss is :  tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
            "Epoch 250 loss is :  tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
            "Epoch 260 loss is :  tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
            "Epoch 270 loss is :  tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
            "Epoch 280 loss is :  tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
            "Epoch 290 loss is :  tensor(0.1294, grad_fn=<MseLossBackward0>)\n",
            "Best Validation Epoch:  64\n",
            "Best Validation Loss:  tensor(3.8746)\n"
          ]
        }
      ]
    }
  ]
}