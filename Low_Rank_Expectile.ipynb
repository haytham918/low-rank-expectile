{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1IfzZyH0SMUQhtWlt6FlyzgemIO6JtiHl",
      "authorship_tag": "ABX9TyND1ce627BZE7pCcoCjABOG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haytham918/low-rank-expectile/blob/main/Low_Rank_Expectile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "saCLAZMMxjWH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib as plot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"/content/drive/MyDrive/low-rank-expectile/heartrate_seconds_merged.csv\")\n",
        "# Convert the 'Time' column to datetime format\n",
        "df['Time'] = pd.to_datetime(df['Time'])\n"
      ],
      "metadata": {
        "id": "IXJmn6cF1342"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_tenmin_df = df.groupby(['Id', pd.Grouper(key='Time', freq='5T')])['Value'].mean().unstack()\n",
        "user_tenmin_matrix = user_tenmin_df.values\n",
        "# print(user_tenmin_df)"
      ],
      "metadata": {
        "id": "U0Y7-t7pCfQ0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_matrix = np.isnan(user_tenmin_matrix)\n",
        "print(\"Matrix Entry Number: \", user_tenmin_matrix.shape[0] * user_tenmin_matrix.shape[1])\n",
        "print(\"Nan Count: \", np.sum(nan_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHynihEq3T9Z",
        "outputId": "ee6cd3e7-9a39-406b-c1ec-76a24ad63b73"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Entry Number:  123718\n",
            "Nan Count:  56200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into trainig/validation and exclude missing values\n",
        "train_data, val_data, train_mask, val_mask = train_test_split(user_tenmin_matrix, nan_matrix, test_size=0.2, random_state=445)\n",
        "\n",
        "\n",
        "# Create Tensors based on train/val data\n",
        "train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
        "val_tensor = torch.tensor(val_data, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Model definition\n",
        "class LRModel(nn.Module):\n",
        "  def __init__(self, number_users, number_times, rank):\n",
        "    super().__init__()\n",
        "    self.user_factors = nn.Embedding(number_users, rank)\n",
        "    self.times_factors = nn.Embedding(number_times, rank)\n",
        "\n",
        "    self.user_bias = nn.Embedding(number_users, 1)\n",
        "    self.times_bias = nn.Embedding(number_times, 1)\n",
        "\n",
        "    # Initializing the bias terms to zeros\n",
        "    self.user_bias.weight.data.fill_(0.)\n",
        "    self.times_bias.weight.data.fill_(0.)\n",
        "\n",
        "\n",
        "  # Define forward propagation\n",
        "  def forward(self, user, times):\n",
        "    # print(self.user_factors(user).shape)\n",
        "    # print(self.times_factors(times).shape)\n",
        "    pred = self.user_factors(user) * self.times_factors(times)\n",
        "    pred = pred.sum(1, keepdim=False)\n",
        "    pred += self.user_bias(user).squeeze() + self.times_bias(times).squeeze()\n",
        "    return pred\n",
        "\n",
        "# Define Loss function excluding missing values\n",
        "def loss_func(predicted, actual, mask):\n",
        "    # print(predicted.shape, actual.shape, mask.shape)\n",
        "    invert_mask = ~mask\n",
        "    # print(predicted[invert_mask].shape, actual[invert_mask].shape)\n",
        "    loss = nn.MSELoss()\n",
        "    return loss(predicted[invert_mask].view(-1), actual[invert_mask].view(-1))\n",
        "\n",
        "# Define parameters in our case\n",
        "number_users, number_times = train_data.shape\n",
        "rank = 5\n",
        "model = LRModel(number_users, number_times, rank)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Epochs and CheckpointPath\n",
        "number_epochs = 800\n",
        "\n",
        "global_best_loss = float('inf')\n",
        "best_epoch = 0\n",
        "\n",
        "print(val_mask.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWwzjaBHWdVR",
        "outputId": "f0a60228-0269-4e35-f8c7-61be04e3851c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 8837)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "for epoch in range(number_epochs):\n",
        "  user_indices = torch.arange(number_users).repeat_interleave(number_times)\n",
        "  time_indices = torch.arange(number_times).repeat(number_users)\n",
        "  output = model(user_indices, time_indices)\n",
        "\n",
        "  # Calculate loss\n",
        "  train_tensor_flat = train_tensor.view(-1)\n",
        "  train_mask_flat = train_mask.reshape(-1)\n",
        "  training_loss = loss_func(output, train_tensor_flat, train_mask_flat)\n",
        "\n",
        "  # Backward\n",
        "  optimizer.zero_grad()\n",
        "  training_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    validation_num_user, validation_num_times = val_tensor.shape\n",
        "    validation_user_indices = torch.arange(validation_num_user).repeat_interleave(validation_num_times)\n",
        "    validation_time_indices = torch.arange(validation_num_times).repeat(validation_num_user)\n",
        "    validation_output = model(validation_user_indices, validation_time_indices)\n",
        "    validation_loss = loss_func(validation_output, val_tensor.view(-1), val_mask.reshape(-1))\n",
        "\n",
        "  print(f\"Epoch [{epoch + 1}/{number_epochs}]: Training Loss: {training_loss.item()}; Validation Loss: {validation_loss.item()}\")\n",
        "\n",
        "  if validation_loss < global_best_loss:\n",
        "      global_best_loss = validation_loss\n",
        "      best_epoch = epoch\n",
        "      torch.save({\"Epoch\": epoch, \"Model_state_dict\": model.state_dict(), \"Optimizer_state_dict\": optimizer.state_dict(), \"Loss\": validation_loss},\n",
        "                 f\"/content/drive/MyDrive/low-rank-expectile/checkpoints/model_checkpoint_epoch{epoch}.pt\")\n",
        "\n",
        "print(\"Best Validation Epoch: \", best_epoch + 1)\n",
        "print(\"Best Validation Loss: \", global_best_loss)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekyhksG0W7lA",
        "outputId": "049a3930-6255-4a8c-a44b-eaaa88466240"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/800]: Training Loss: 5884.9814453125; Validation Loss: 5305.85400390625\n",
            "Epoch [2/800]: Training Loss: 5879.509765625; Validation Loss: 5302.99609375\n",
            "Epoch [3/800]: Training Loss: 5873.9970703125; Validation Loss: 5300.0498046875\n",
            "Epoch [4/800]: Training Loss: 5868.4208984375; Validation Loss: 5297.00439453125\n",
            "Epoch [5/800]: Training Loss: 5862.76171875; Validation Loss: 5293.857421875\n",
            "Epoch [6/800]: Training Loss: 5857.005859375; Validation Loss: 5290.60400390625\n",
            "Epoch [7/800]: Training Loss: 5851.1396484375; Validation Loss: 5287.24072265625\n",
            "Epoch [8/800]: Training Loss: 5845.1552734375; Validation Loss: 5283.7626953125\n",
            "Epoch [9/800]: Training Loss: 5839.041015625; Validation Loss: 5280.1669921875\n",
            "Epoch [10/800]: Training Loss: 5832.7919921875; Validation Loss: 5276.4482421875\n",
            "Epoch [11/800]: Training Loss: 5826.39892578125; Validation Loss: 5272.60205078125\n",
            "Epoch [12/800]: Training Loss: 5819.857421875; Validation Loss: 5268.625\n",
            "Epoch [13/800]: Training Loss: 5813.16015625; Validation Loss: 5264.5087890625\n",
            "Epoch [14/800]: Training Loss: 5806.30078125; Validation Loss: 5260.25146484375\n",
            "Epoch [15/800]: Training Loss: 5799.2734375; Validation Loss: 5255.8466796875\n",
            "Epoch [16/800]: Training Loss: 5792.0732421875; Validation Loss: 5251.2890625\n",
            "Epoch [17/800]: Training Loss: 5784.6923828125; Validation Loss: 5246.57373046875\n",
            "Epoch [18/800]: Training Loss: 5777.126953125; Validation Loss: 5241.69482421875\n",
            "Epoch [19/800]: Training Loss: 5769.36865234375; Validation Loss: 5236.64794921875\n",
            "Epoch [20/800]: Training Loss: 5761.4130859375; Validation Loss: 5231.427734375\n",
            "Epoch [21/800]: Training Loss: 5753.25537109375; Validation Loss: 5226.029296875\n",
            "Epoch [22/800]: Training Loss: 5744.88916015625; Validation Loss: 5220.4482421875\n",
            "Epoch [23/800]: Training Loss: 5736.3095703125; Validation Loss: 5214.677734375\n",
            "Epoch [24/800]: Training Loss: 5727.51123046875; Validation Loss: 5208.71484375\n",
            "Epoch [25/800]: Training Loss: 5718.490234375; Validation Loss: 5202.5546875\n",
            "Epoch [26/800]: Training Loss: 5709.240234375; Validation Loss: 5196.19091796875\n",
            "Epoch [27/800]: Training Loss: 5699.75634765625; Validation Loss: 5189.62060546875\n",
            "Epoch [28/800]: Training Loss: 5690.03564453125; Validation Loss: 5182.83984375\n",
            "Epoch [29/800]: Training Loss: 5680.0732421875; Validation Loss: 5175.84228515625\n",
            "Epoch [30/800]: Training Loss: 5669.86279296875; Validation Loss: 5168.62646484375\n",
            "Epoch [31/800]: Training Loss: 5659.40380859375; Validation Loss: 5161.185546875\n",
            "Epoch [32/800]: Training Loss: 5648.68994140625; Validation Loss: 5153.5166015625\n",
            "Epoch [33/800]: Training Loss: 5637.71923828125; Validation Loss: 5145.61669921875\n",
            "Epoch [34/800]: Training Loss: 5626.486328125; Validation Loss: 5137.4814453125\n",
            "Epoch [35/800]: Training Loss: 5614.990234375; Validation Loss: 5129.10693359375\n",
            "Epoch [36/800]: Training Loss: 5603.22509765625; Validation Loss: 5120.49072265625\n",
            "Epoch [37/800]: Training Loss: 5591.1904296875; Validation Loss: 5111.6298828125\n",
            "Epoch [38/800]: Training Loss: 5578.8818359375; Validation Loss: 5102.5205078125\n",
            "Epoch [39/800]: Training Loss: 5566.29833984375; Validation Loss: 5093.16015625\n",
            "Epoch [40/800]: Training Loss: 5553.43505859375; Validation Loss: 5083.5458984375\n",
            "Epoch [41/800]: Training Loss: 5540.29248046875; Validation Loss: 5073.67529296875\n",
            "Epoch [42/800]: Training Loss: 5526.86669921875; Validation Loss: 5063.5458984375\n",
            "Epoch [43/800]: Training Loss: 5513.15625; Validation Loss: 5053.15576171875\n",
            "Epoch [44/800]: Training Loss: 5499.15966796875; Validation Loss: 5042.50244140625\n",
            "Epoch [45/800]: Training Loss: 5484.87548828125; Validation Loss: 5031.58447265625\n",
            "Epoch [46/800]: Training Loss: 5470.30126953125; Validation Loss: 5020.3994140625\n",
            "Epoch [47/800]: Training Loss: 5455.43701171875; Validation Loss: 5008.9453125\n",
            "Epoch [48/800]: Training Loss: 5440.28125; Validation Loss: 4997.2216796875\n",
            "Epoch [49/800]: Training Loss: 5424.83349609375; Validation Loss: 4985.2265625\n",
            "Epoch [50/800]: Training Loss: 5409.09228515625; Validation Loss: 4972.958984375\n",
            "Epoch [51/800]: Training Loss: 5393.0576171875; Validation Loss: 4960.41796875\n",
            "Epoch [52/800]: Training Loss: 5376.72900390625; Validation Loss: 4947.60205078125\n",
            "Epoch [53/800]: Training Loss: 5360.10498046875; Validation Loss: 4934.51025390625\n",
            "Epoch [54/800]: Training Loss: 5343.18798828125; Validation Loss: 4921.14306640625\n",
            "Epoch [55/800]: Training Loss: 5325.9755859375; Validation Loss: 4907.4990234375\n",
            "Epoch [56/800]: Training Loss: 5308.46875; Validation Loss: 4893.578125\n",
            "Epoch [57/800]: Training Loss: 5290.66845703125; Validation Loss: 4879.3798828125\n",
            "Epoch [58/800]: Training Loss: 5272.57421875; Validation Loss: 4864.90478515625\n",
            "Epoch [59/800]: Training Loss: 5254.1875; Validation Loss: 4850.1513671875\n",
            "Epoch [60/800]: Training Loss: 5235.50830078125; Validation Loss: 4835.12109375\n",
            "Epoch [61/800]: Training Loss: 5216.537109375; Validation Loss: 4819.81396484375\n",
            "Epoch [62/800]: Training Loss: 5197.2763671875; Validation Loss: 4804.22998046875\n",
            "Epoch [63/800]: Training Loss: 5177.7265625; Validation Loss: 4788.3701171875\n",
            "Epoch [64/800]: Training Loss: 5157.8876953125; Validation Loss: 4772.234375\n",
            "Epoch [65/800]: Training Loss: 5137.7626953125; Validation Loss: 4755.82373046875\n",
            "Epoch [66/800]: Training Loss: 5117.3525390625; Validation Loss: 4739.13916015625\n",
            "Epoch [67/800]: Training Loss: 5096.65869140625; Validation Loss: 4722.18212890625\n",
            "Epoch [68/800]: Training Loss: 5075.68310546875; Validation Loss: 4704.95263671875\n",
            "Epoch [69/800]: Training Loss: 5054.427734375; Validation Loss: 4687.4521484375\n",
            "Epoch [70/800]: Training Loss: 5032.8935546875; Validation Loss: 4669.68212890625\n",
            "Epoch [71/800]: Training Loss: 5011.08349609375; Validation Loss: 4651.64453125\n",
            "Epoch [72/800]: Training Loss: 4988.99951171875; Validation Loss: 4633.34033203125\n",
            "Epoch [73/800]: Training Loss: 4966.6435546875; Validation Loss: 4614.77099609375\n",
            "Epoch [74/800]: Training Loss: 4944.01904296875; Validation Loss: 4595.93798828125\n",
            "Epoch [75/800]: Training Loss: 4921.126953125; Validation Loss: 4576.84423828125\n",
            "Epoch [76/800]: Training Loss: 4897.97119140625; Validation Loss: 4557.48974609375\n",
            "Epoch [77/800]: Training Loss: 4874.55322265625; Validation Loss: 4537.8779296875\n",
            "Epoch [78/800]: Training Loss: 4850.87646484375; Validation Loss: 4518.01123046875\n",
            "Epoch [79/800]: Training Loss: 4826.94482421875; Validation Loss: 4497.890625\n",
            "Epoch [80/800]: Training Loss: 4802.759765625; Validation Loss: 4477.51953125\n",
            "Epoch [81/800]: Training Loss: 4778.32373046875; Validation Loss: 4456.89892578125\n",
            "Epoch [82/800]: Training Loss: 4753.6416015625; Validation Loss: 4436.0322265625\n",
            "Epoch [83/800]: Training Loss: 4728.7158203125; Validation Loss: 4414.92236328125\n",
            "Epoch [84/800]: Training Loss: 4703.5498046875; Validation Loss: 4393.5712890625\n",
            "Epoch [85/800]: Training Loss: 4678.146484375; Validation Loss: 4371.98193359375\n",
            "Epoch [86/800]: Training Loss: 4652.509765625; Validation Loss: 4350.15673828125\n",
            "Epoch [87/800]: Training Loss: 4626.64306640625; Validation Loss: 4328.09912109375\n",
            "Epoch [88/800]: Training Loss: 4600.5498046875; Validation Loss: 4305.8115234375\n",
            "Epoch [89/800]: Training Loss: 4574.23486328125; Validation Loss: 4283.29736328125\n",
            "Epoch [90/800]: Training Loss: 4547.7001953125; Validation Loss: 4260.5595703125\n",
            "Epoch [91/800]: Training Loss: 4520.95068359375; Validation Loss: 4237.60107421875\n",
            "Epoch [92/800]: Training Loss: 4493.98974609375; Validation Loss: 4214.42578125\n",
            "Epoch [93/800]: Training Loss: 4466.82275390625; Validation Loss: 4191.03662109375\n",
            "Epoch [94/800]: Training Loss: 4439.45166015625; Validation Loss: 4167.43701171875\n",
            "Epoch [95/800]: Training Loss: 4411.88232421875; Validation Loss: 4143.63037109375\n",
            "Epoch [96/800]: Training Loss: 4384.1171875; Validation Loss: 4119.62060546875\n",
            "Epoch [97/800]: Training Loss: 4356.16259765625; Validation Loss: 4095.410400390625\n",
            "Epoch [98/800]: Training Loss: 4328.02099609375; Validation Loss: 4071.00390625\n",
            "Epoch [99/800]: Training Loss: 4299.69775390625; Validation Loss: 4046.406005859375\n",
            "Epoch [100/800]: Training Loss: 4271.197265625; Validation Loss: 4021.6181640625\n",
            "Epoch [101/800]: Training Loss: 4242.52392578125; Validation Loss: 3996.646484375\n",
            "Epoch [102/800]: Training Loss: 4213.681640625; Validation Loss: 3971.492919921875\n",
            "Epoch [103/800]: Training Loss: 4184.67529296875; Validation Loss: 3946.16357421875\n",
            "Epoch [104/800]: Training Loss: 4155.51025390625; Validation Loss: 3920.660400390625\n",
            "Epoch [105/800]: Training Loss: 4126.1904296875; Validation Loss: 3894.989013671875\n",
            "Epoch [106/800]: Training Loss: 4096.72119140625; Validation Loss: 3869.152099609375\n",
            "Epoch [107/800]: Training Loss: 4067.106201171875; Validation Loss: 3843.155029296875\n",
            "Epoch [108/800]: Training Loss: 4037.35107421875; Validation Loss: 3817.001220703125\n",
            "Epoch [109/800]: Training Loss: 4007.460693359375; Validation Loss: 3790.69580078125\n",
            "Epoch [110/800]: Training Loss: 3977.440185546875; Validation Loss: 3764.2421875\n",
            "Epoch [111/800]: Training Loss: 3947.292724609375; Validation Loss: 3737.64501953125\n",
            "Epoch [112/800]: Training Loss: 3917.02587890625; Validation Loss: 3710.90869140625\n",
            "Epoch [113/800]: Training Loss: 3886.64306640625; Validation Loss: 3684.037841796875\n",
            "Epoch [114/800]: Training Loss: 3856.149658203125; Validation Loss: 3657.036865234375\n",
            "Epoch [115/800]: Training Loss: 3825.5498046875; Validation Loss: 3629.91064453125\n",
            "Epoch [116/800]: Training Loss: 3794.85009765625; Validation Loss: 3602.6630859375\n",
            "Epoch [117/800]: Training Loss: 3764.054931640625; Validation Loss: 3575.298583984375\n",
            "Epoch [118/800]: Training Loss: 3733.170166015625; Validation Loss: 3547.822509765625\n",
            "Epoch [119/800]: Training Loss: 3702.19970703125; Validation Loss: 3520.239013671875\n",
            "Epoch [120/800]: Training Loss: 3671.149658203125; Validation Loss: 3492.552978515625\n",
            "Epoch [121/800]: Training Loss: 3640.024658203125; Validation Loss: 3464.769287109375\n",
            "Epoch [122/800]: Training Loss: 3608.83056640625; Validation Loss: 3436.892578125\n",
            "Epoch [123/800]: Training Loss: 3577.572021484375; Validation Loss: 3408.927001953125\n",
            "Epoch [124/800]: Training Loss: 3546.254638671875; Validation Loss: 3380.878173828125\n",
            "Epoch [125/800]: Training Loss: 3514.88330078125; Validation Loss: 3352.75\n",
            "Epoch [126/800]: Training Loss: 3483.46435546875; Validation Loss: 3324.54833984375\n",
            "Epoch [127/800]: Training Loss: 3452.00146484375; Validation Loss: 3296.277587890625\n",
            "Epoch [128/800]: Training Loss: 3420.500732421875; Validation Loss: 3267.9423828125\n",
            "Epoch [129/800]: Training Loss: 3388.96826171875; Validation Loss: 3239.54736328125\n",
            "Epoch [130/800]: Training Loss: 3357.408447265625; Validation Loss: 3211.09814453125\n",
            "Epoch [131/800]: Training Loss: 3325.826171875; Validation Loss: 3182.59912109375\n",
            "Epoch [132/800]: Training Loss: 3294.227783203125; Validation Loss: 3154.05517578125\n",
            "Epoch [133/800]: Training Loss: 3262.61767578125; Validation Loss: 3125.4716796875\n",
            "Epoch [134/800]: Training Loss: 3231.002197265625; Validation Loss: 3096.853515625\n",
            "Epoch [135/800]: Training Loss: 3199.38525390625; Validation Loss: 3068.20458984375\n",
            "Epoch [136/800]: Training Loss: 3167.773681640625; Validation Loss: 3039.53076171875\n",
            "Epoch [137/800]: Training Loss: 3136.171630859375; Validation Loss: 3010.83740234375\n",
            "Epoch [138/800]: Training Loss: 3104.5849609375; Validation Loss: 2982.12841796875\n",
            "Epoch [139/800]: Training Loss: 3073.018798828125; Validation Loss: 2953.4091796875\n",
            "Epoch [140/800]: Training Loss: 3041.477783203125; Validation Loss: 2924.684326171875\n",
            "Epoch [141/800]: Training Loss: 3009.96728515625; Validation Loss: 2895.958984375\n",
            "Epoch [142/800]: Training Loss: 2978.49365234375; Validation Loss: 2867.23828125\n",
            "Epoch [143/800]: Training Loss: 2947.06103515625; Validation Loss: 2838.527099609375\n",
            "Epoch [144/800]: Training Loss: 2915.6748046875; Validation Loss: 2809.829833984375\n",
            "Epoch [145/800]: Training Loss: 2884.339599609375; Validation Loss: 2781.151611328125\n",
            "Epoch [146/800]: Training Loss: 2853.061279296875; Validation Loss: 2752.4970703125\n",
            "Epoch [147/800]: Training Loss: 2821.8447265625; Validation Loss: 2723.871826171875\n",
            "Epoch [148/800]: Training Loss: 2790.695068359375; Validation Loss: 2695.27978515625\n",
            "Epoch [149/800]: Training Loss: 2759.616943359375; Validation Loss: 2666.726318359375\n",
            "Epoch [150/800]: Training Loss: 2728.615478515625; Validation Loss: 2638.216064453125\n",
            "Epoch [151/800]: Training Loss: 2697.696044921875; Validation Loss: 2609.753662109375\n",
            "Epoch [152/800]: Training Loss: 2666.863037109375; Validation Loss: 2581.34423828125\n",
            "Epoch [153/800]: Training Loss: 2636.121337890625; Validation Loss: 2552.99169921875\n",
            "Epoch [154/800]: Training Loss: 2605.47607421875; Validation Loss: 2524.701416015625\n",
            "Epoch [155/800]: Training Loss: 2574.931884765625; Validation Loss: 2496.477783203125\n",
            "Epoch [156/800]: Training Loss: 2544.492919921875; Validation Loss: 2468.3251953125\n",
            "Epoch [157/800]: Training Loss: 2514.1650390625; Validation Loss: 2440.249267578125\n",
            "Epoch [158/800]: Training Loss: 2483.9521484375; Validation Loss: 2412.253662109375\n",
            "Epoch [159/800]: Training Loss: 2453.858642578125; Validation Loss: 2384.342529296875\n",
            "Epoch [160/800]: Training Loss: 2423.8896484375; Validation Loss: 2356.521240234375\n",
            "Epoch [161/800]: Training Loss: 2394.04931640625; Validation Loss: 2328.793701171875\n",
            "Epoch [162/800]: Training Loss: 2364.341796875; Validation Loss: 2301.164306640625\n",
            "Epoch [163/800]: Training Loss: 2334.77197265625; Validation Loss: 2273.637939453125\n",
            "Epoch [164/800]: Training Loss: 2305.343994140625; Validation Loss: 2246.218017578125\n",
            "Epoch [165/800]: Training Loss: 2276.06201171875; Validation Loss: 2218.909423828125\n",
            "Epoch [166/800]: Training Loss: 2246.930419921875; Validation Loss: 2191.716552734375\n",
            "Epoch [167/800]: Training Loss: 2217.952880859375; Validation Loss: 2164.642822265625\n",
            "Epoch [168/800]: Training Loss: 2189.134033203125; Validation Loss: 2137.692626953125\n",
            "Epoch [169/800]: Training Loss: 2160.477294921875; Validation Loss: 2110.870849609375\n",
            "Epoch [170/800]: Training Loss: 2131.987060546875; Validation Loss: 2084.18017578125\n",
            "Epoch [171/800]: Training Loss: 2103.6669921875; Validation Loss: 2057.625244140625\n",
            "Epoch [172/800]: Training Loss: 2075.520751953125; Validation Loss: 2031.209716796875\n",
            "Epoch [173/800]: Training Loss: 2047.55224609375; Validation Loss: 2004.93798828125\n",
            "Epoch [174/800]: Training Loss: 2019.7647705078125; Validation Loss: 1978.8131103515625\n",
            "Epoch [175/800]: Training Loss: 1992.1619873046875; Validation Loss: 1952.839111328125\n",
            "Epoch [176/800]: Training Loss: 1964.747802734375; Validation Loss: 1927.01953125\n",
            "Epoch [177/800]: Training Loss: 1937.5250244140625; Validation Loss: 1901.3580322265625\n",
            "Epoch [178/800]: Training Loss: 1910.49755859375; Validation Loss: 1875.8580322265625\n",
            "Epoch [179/800]: Training Loss: 1883.66796875; Validation Loss: 1850.5230712890625\n",
            "Epoch [180/800]: Training Loss: 1857.0396728515625; Validation Loss: 1825.3565673828125\n",
            "Epoch [181/800]: Training Loss: 1830.6160888671875; Validation Loss: 1800.361572265625\n",
            "Epoch [182/800]: Training Loss: 1804.399658203125; Validation Loss: 1775.541748046875\n",
            "Epoch [183/800]: Training Loss: 1778.3936767578125; Validation Loss: 1750.8997802734375\n",
            "Epoch [184/800]: Training Loss: 1752.6005859375; Validation Loss: 1726.438720703125\n",
            "Epoch [185/800]: Training Loss: 1727.0233154296875; Validation Loss: 1702.162353515625\n",
            "Epoch [186/800]: Training Loss: 1701.6644287109375; Validation Loss: 1678.0728759765625\n",
            "Epoch [187/800]: Training Loss: 1676.5267333984375; Validation Loss: 1654.1732177734375\n",
            "Epoch [188/800]: Training Loss: 1651.6119384765625; Validation Loss: 1630.466552734375\n",
            "Epoch [189/800]: Training Loss: 1626.9232177734375; Validation Loss: 1606.955078125\n",
            "Epoch [190/800]: Training Loss: 1602.4625244140625; Validation Loss: 1583.641845703125\n",
            "Epoch [191/800]: Training Loss: 1578.2318115234375; Validation Loss: 1560.5291748046875\n",
            "Epoch [192/800]: Training Loss: 1554.2332763671875; Validation Loss: 1537.6195068359375\n",
            "Epoch [193/800]: Training Loss: 1530.4691162109375; Validation Loss: 1514.9154052734375\n",
            "Epoch [194/800]: Training Loss: 1506.94091796875; Validation Loss: 1492.4189453125\n",
            "Epoch [195/800]: Training Loss: 1483.650634765625; Validation Loss: 1470.132568359375\n",
            "Epoch [196/800]: Training Loss: 1460.5999755859375; Validation Loss: 1448.05810546875\n",
            "Epoch [197/800]: Training Loss: 1437.7904052734375; Validation Loss: 1426.19775390625\n",
            "Epoch [198/800]: Training Loss: 1415.2236328125; Validation Loss: 1404.5537109375\n",
            "Epoch [199/800]: Training Loss: 1392.9007568359375; Validation Loss: 1383.12744140625\n",
            "Epoch [200/800]: Training Loss: 1370.8232421875; Validation Loss: 1361.9208984375\n",
            "Epoch [201/800]: Training Loss: 1348.9921875; Validation Loss: 1340.93603515625\n",
            "Epoch [202/800]: Training Loss: 1327.4088134765625; Validation Loss: 1320.1737060546875\n",
            "Epoch [203/800]: Training Loss: 1306.0740966796875; Validation Loss: 1299.63623046875\n",
            "Epoch [204/800]: Training Loss: 1284.989013671875; Validation Loss: 1279.32470703125\n",
            "Epoch [205/800]: Training Loss: 1264.154296875; Validation Loss: 1259.24072265625\n",
            "Epoch [206/800]: Training Loss: 1243.5706787109375; Validation Loss: 1239.385009765625\n",
            "Epoch [207/800]: Training Loss: 1223.2386474609375; Validation Loss: 1219.7591552734375\n",
            "Epoch [208/800]: Training Loss: 1203.158935546875; Validation Loss: 1200.364013671875\n",
            "Epoch [209/800]: Training Loss: 1183.331787109375; Validation Loss: 1181.200927734375\n",
            "Epoch [210/800]: Training Loss: 1163.758056640625; Validation Loss: 1162.2705078125\n",
            "Epoch [211/800]: Training Loss: 1144.4371337890625; Validation Loss: 1143.573486328125\n",
            "Epoch [212/800]: Training Loss: 1125.3697509765625; Validation Loss: 1125.11083984375\n",
            "Epoch [213/800]: Training Loss: 1106.5557861328125; Validation Loss: 1106.883056640625\n",
            "Epoch [214/800]: Training Loss: 1087.9952392578125; Validation Loss: 1088.8907470703125\n",
            "Epoch [215/800]: Training Loss: 1069.68798828125; Validation Loss: 1071.13427734375\n",
            "Epoch [216/800]: Training Loss: 1051.633544921875; Validation Loss: 1053.6141357421875\n",
            "Epoch [217/800]: Training Loss: 1033.8321533203125; Validation Loss: 1036.3304443359375\n",
            "Epoch [218/800]: Training Loss: 1016.28271484375; Validation Loss: 1019.2837524414062\n",
            "Epoch [219/800]: Training Loss: 998.9852905273438; Validation Loss: 1002.4737548828125\n",
            "Epoch [220/800]: Training Loss: 981.9390258789062; Validation Loss: 985.90087890625\n",
            "Epoch [221/800]: Training Loss: 965.1435546875; Validation Loss: 969.5646362304688\n",
            "Epoch [222/800]: Training Loss: 948.5977172851562; Validation Loss: 953.4653930664062\n",
            "Epoch [223/800]: Training Loss: 932.3009643554688; Validation Loss: 937.6024169921875\n",
            "Epoch [224/800]: Training Loss: 916.2522583007812; Validation Loss: 921.9757690429688\n",
            "Epoch [225/800]: Training Loss: 900.4506225585938; Validation Loss: 906.5849609375\n",
            "Epoch [226/800]: Training Loss: 884.8950805664062; Validation Loss: 891.4295043945312\n",
            "Epoch [227/800]: Training Loss: 869.5842895507812; Validation Loss: 876.5089721679688\n",
            "Epoch [228/800]: Training Loss: 854.5172119140625; Validation Loss: 861.8226928710938\n",
            "Epoch [229/800]: Training Loss: 839.6925659179688; Validation Loss: 847.369873046875\n",
            "Epoch [230/800]: Training Loss: 825.1087646484375; Validation Loss: 833.1497802734375\n",
            "Epoch [231/800]: Training Loss: 810.7646484375; Validation Loss: 819.1617431640625\n",
            "Epoch [232/800]: Training Loss: 796.6583862304688; Validation Loss: 805.4046020507812\n",
            "Epoch [233/800]: Training Loss: 782.7887573242188; Validation Loss: 791.8775024414062\n",
            "Epoch [234/800]: Training Loss: 769.1538696289062; Validation Loss: 778.5794677734375\n",
            "Epoch [235/800]: Training Loss: 755.7520141601562; Validation Loss: 765.5093383789062\n",
            "Epoch [236/800]: Training Loss: 742.5816040039062; Validation Loss: 752.6657104492188\n",
            "Epoch [237/800]: Training Loss: 729.6406860351562; Validation Loss: 740.0475463867188\n",
            "Epoch [238/800]: Training Loss: 716.9273681640625; Validation Loss: 727.6535034179688\n",
            "Epoch [239/800]: Training Loss: 704.4398803710938; Validation Loss: 715.4821166992188\n",
            "Epoch [240/800]: Training Loss: 692.1759643554688; Validation Loss: 703.5320434570312\n",
            "Epoch [241/800]: Training Loss: 680.1338500976562; Validation Loss: 691.8015747070312\n",
            "Epoch [242/800]: Training Loss: 668.3112182617188; Validation Loss: 680.2894287109375\n",
            "Epoch [243/800]: Training Loss: 656.7060546875; Validation Loss: 668.9938354492188\n",
            "Epoch [244/800]: Training Loss: 645.3162231445312; Validation Loss: 657.9131469726562\n",
            "Epoch [245/800]: Training Loss: 634.1394653320312; Validation Loss: 647.0455322265625\n",
            "Epoch [246/800]: Training Loss: 623.1734008789062; Validation Loss: 636.389404296875\n",
            "Epoch [247/800]: Training Loss: 612.4158935546875; Validation Loss: 625.9427490234375\n",
            "Epoch [248/800]: Training Loss: 601.8646850585938; Validation Loss: 615.703857421875\n",
            "Epoch [249/800]: Training Loss: 591.51708984375; Validation Loss: 605.6707153320312\n",
            "Epoch [250/800]: Training Loss: 581.370849609375; Validation Loss: 595.8414916992188\n",
            "Epoch [251/800]: Training Loss: 571.4237060546875; Validation Loss: 586.2138671875\n",
            "Epoch [252/800]: Training Loss: 561.6730346679688; Validation Loss: 576.7861938476562\n",
            "Epoch [253/800]: Training Loss: 552.1162719726562; Validation Loss: 567.5560913085938\n",
            "Epoch [254/800]: Training Loss: 542.7510986328125; Validation Loss: 558.5216064453125\n",
            "Epoch [255/800]: Training Loss: 533.574951171875; Validation Loss: 549.6804809570312\n",
            "Epoch [256/800]: Training Loss: 524.5851440429688; Validation Loss: 541.0305786132812\n",
            "Epoch [257/800]: Training Loss: 515.7793579101562; Validation Loss: 532.5697021484375\n",
            "Epoch [258/800]: Training Loss: 507.15484619140625; Validation Loss: 524.295654296875\n",
            "Epoch [259/800]: Training Loss: 498.708984375; Validation Loss: 516.2059936523438\n",
            "Epoch [260/800]: Training Loss: 490.4392395019531; Validation Loss: 508.2986145019531\n",
            "Epoch [261/800]: Training Loss: 482.3431091308594; Validation Loss: 500.571044921875\n",
            "Epoch [262/800]: Training Loss: 474.4177551269531; Validation Loss: 493.0209655761719\n",
            "Epoch [263/800]: Training Loss: 466.6607360839844; Validation Loss: 485.6460876464844\n",
            "Epoch [264/800]: Training Loss: 459.0693664550781; Validation Loss: 478.4440002441406\n",
            "Epoch [265/800]: Training Loss: 451.6409912109375; Validation Loss: 471.4122314453125\n",
            "Epoch [266/800]: Training Loss: 444.37298583984375; Validation Loss: 464.548583984375\n",
            "Epoch [267/800]: Training Loss: 437.2627868652344; Validation Loss: 457.8503723144531\n",
            "Epoch [268/800]: Training Loss: 430.30780029296875; Validation Loss: 451.3152770996094\n",
            "Epoch [269/800]: Training Loss: 423.5052795410156; Validation Loss: 444.9408874511719\n",
            "Epoch [270/800]: Training Loss: 416.8527526855469; Validation Loss: 438.7247314453125\n",
            "Epoch [271/800]: Training Loss: 410.3475646972656; Validation Loss: 432.664306640625\n",
            "Epoch [272/800]: Training Loss: 403.98712158203125; Validation Loss: 426.7572021484375\n",
            "Epoch [273/800]: Training Loss: 397.7688903808594; Validation Loss: 421.0009460449219\n",
            "Epoch [274/800]: Training Loss: 391.6902770996094; Validation Loss: 415.39306640625\n",
            "Epoch [275/800]: Training Loss: 385.7487487792969; Validation Loss: 409.93109130859375\n",
            "Epoch [276/800]: Training Loss: 379.94171142578125; Validation Loss: 404.6125793457031\n",
            "Epoch [277/800]: Training Loss: 374.26678466796875; Validation Loss: 399.43505859375\n",
            "Epoch [278/800]: Training Loss: 368.7212829589844; Validation Loss: 394.3960876464844\n",
            "Epoch [279/800]: Training Loss: 363.302978515625; Validation Loss: 389.4931640625\n",
            "Epoch [280/800]: Training Loss: 358.0091857910156; Validation Loss: 384.72393798828125\n",
            "Epoch [281/800]: Training Loss: 352.83758544921875; Validation Loss: 380.0859069824219\n",
            "Epoch [282/800]: Training Loss: 347.7857360839844; Validation Loss: 375.5767517089844\n",
            "Epoch [283/800]: Training Loss: 342.85125732421875; Validation Loss: 371.1938781738281\n",
            "Epoch [284/800]: Training Loss: 338.03179931640625; Validation Loss: 366.9350891113281\n",
            "Epoch [285/800]: Training Loss: 333.3249816894531; Validation Loss: 362.7978210449219\n",
            "Epoch [286/800]: Training Loss: 328.7285461425781; Validation Loss: 358.7798156738281\n",
            "Epoch [287/800]: Training Loss: 324.2401428222656; Validation Loss: 354.8787536621094\n",
            "Epoch [288/800]: Training Loss: 319.8576354980469; Validation Loss: 351.09222412109375\n",
            "Epoch [289/800]: Training Loss: 315.57861328125; Validation Loss: 347.41790771484375\n",
            "Epoch [290/800]: Training Loss: 311.4010009765625; Validation Loss: 343.853515625\n",
            "Epoch [291/800]: Training Loss: 307.3226013183594; Validation Loss: 340.3968200683594\n",
            "Epoch [292/800]: Training Loss: 303.3412780761719; Validation Loss: 337.04547119140625\n",
            "Epoch [293/800]: Training Loss: 299.45489501953125; Validation Loss: 333.7973327636719\n",
            "Epoch [294/800]: Training Loss: 295.661376953125; Validation Loss: 330.65008544921875\n",
            "Epoch [295/800]: Training Loss: 291.9586486816406; Validation Loss: 327.6015930175781\n",
            "Epoch [296/800]: Training Loss: 288.3447570800781; Validation Loss: 324.64971923828125\n",
            "Epoch [297/800]: Training Loss: 284.8176574707031; Validation Loss: 321.7922668457031\n",
            "Epoch [298/800]: Training Loss: 281.3753967285156; Validation Loss: 319.02716064453125\n",
            "Epoch [299/800]: Training Loss: 278.0160827636719; Validation Loss: 316.35223388671875\n",
            "Epoch [300/800]: Training Loss: 274.7377624511719; Validation Loss: 313.7655029296875\n",
            "Epoch [301/800]: Training Loss: 271.5386657714844; Validation Loss: 311.2648010253906\n",
            "Epoch [302/800]: Training Loss: 268.4169006347656; Validation Loss: 308.8482971191406\n",
            "Epoch [303/800]: Training Loss: 265.37066650390625; Validation Loss: 306.51385498046875\n",
            "Epoch [304/800]: Training Loss: 262.398193359375; Validation Loss: 304.2595520019531\n",
            "Epoch [305/800]: Training Loss: 259.497802734375; Validation Loss: 302.0835266113281\n",
            "Epoch [306/800]: Training Loss: 256.6677551269531; Validation Loss: 299.9837646484375\n",
            "Epoch [307/800]: Training Loss: 253.9063720703125; Validation Loss: 297.95843505859375\n",
            "Epoch [308/800]: Training Loss: 251.21205139160156; Validation Loss: 296.0057373046875\n",
            "Epoch [309/800]: Training Loss: 248.5832061767578; Validation Loss: 294.123779296875\n",
            "Epoch [310/800]: Training Loss: 246.0181884765625; Validation Loss: 292.310791015625\n",
            "Epoch [311/800]: Training Loss: 243.5155029296875; Validation Loss: 290.5650634765625\n",
            "Epoch [312/800]: Training Loss: 241.0736541748047; Validation Loss: 288.88482666015625\n",
            "Epoch [313/800]: Training Loss: 238.69113159179688; Validation Loss: 287.2684020996094\n",
            "Epoch [314/800]: Training Loss: 236.3665313720703; Validation Loss: 285.7140808105469\n",
            "Epoch [315/800]: Training Loss: 234.09840393066406; Validation Loss: 284.22027587890625\n",
            "Epoch [316/800]: Training Loss: 231.88534545898438; Validation Loss: 282.7852783203125\n",
            "Epoch [317/800]: Training Loss: 229.72601318359375; Validation Loss: 281.4076843261719\n",
            "Epoch [318/800]: Training Loss: 227.6190948486328; Validation Loss: 280.08575439453125\n",
            "Epoch [319/800]: Training Loss: 225.56329345703125; Validation Loss: 278.8180847167969\n",
            "Epoch [320/800]: Training Loss: 223.5573272705078; Validation Loss: 277.6031188964844\n",
            "Epoch [321/800]: Training Loss: 221.5999298095703; Validation Loss: 276.43939208984375\n",
            "Epoch [322/800]: Training Loss: 219.68994140625; Validation Loss: 275.3255310058594\n",
            "Epoch [323/800]: Training Loss: 217.82615661621094; Validation Loss: 274.2601013183594\n",
            "Epoch [324/800]: Training Loss: 216.00738525390625; Validation Loss: 273.2416687011719\n",
            "Epoch [325/800]: Training Loss: 214.23251342773438; Validation Loss: 272.26898193359375\n",
            "Epoch [326/800]: Training Loss: 212.5004425048828; Validation Loss: 271.3406677246094\n",
            "Epoch [327/800]: Training Loss: 210.81015014648438; Validation Loss: 270.4554443359375\n",
            "Epoch [328/800]: Training Loss: 209.16046142578125; Validation Loss: 269.61199951171875\n",
            "Epoch [329/800]: Training Loss: 207.55044555664062; Validation Loss: 268.8091735839844\n",
            "Epoch [330/800]: Training Loss: 205.9790496826172; Validation Loss: 268.0456848144531\n",
            "Epoch [331/800]: Training Loss: 204.4453125; Validation Loss: 267.3204650878906\n",
            "Epoch [332/800]: Training Loss: 202.94827270507812; Validation Loss: 266.6321716308594\n",
            "Epoch [333/800]: Training Loss: 201.487060546875; Validation Loss: 265.9798278808594\n",
            "Epoch [334/800]: Training Loss: 200.06063842773438; Validation Loss: 265.3623046875\n",
            "Epoch [335/800]: Training Loss: 198.66824340820312; Validation Loss: 264.7785339355469\n",
            "Epoch [336/800]: Training Loss: 197.30894470214844; Validation Loss: 264.2274169921875\n",
            "Epoch [337/800]: Training Loss: 195.98187255859375; Validation Loss: 263.70794677734375\n",
            "Epoch [338/800]: Training Loss: 194.6863250732422; Validation Loss: 263.21917724609375\n",
            "Epoch [339/800]: Training Loss: 193.42138671875; Validation Loss: 262.760009765625\n",
            "Epoch [340/800]: Training Loss: 192.1863250732422; Validation Loss: 262.32958984375\n",
            "Epoch [341/800]: Training Loss: 190.98036193847656; Validation Loss: 261.92694091796875\n",
            "Epoch [342/800]: Training Loss: 189.8027801513672; Validation Loss: 261.5511779785156\n",
            "Epoch [343/800]: Training Loss: 188.65280151367188; Validation Loss: 261.2013854980469\n",
            "Epoch [344/800]: Training Loss: 187.52981567382812; Validation Loss: 260.8768005371094\n",
            "Epoch [345/800]: Training Loss: 186.4330596923828; Validation Loss: 260.57647705078125\n",
            "Epoch [346/800]: Training Loss: 185.36187744140625; Validation Loss: 260.2996520996094\n",
            "Epoch [347/800]: Training Loss: 184.315673828125; Validation Loss: 260.04547119140625\n",
            "Epoch [348/800]: Training Loss: 183.2937469482422; Validation Loss: 259.813232421875\n",
            "Epoch [349/800]: Training Loss: 182.29551696777344; Validation Loss: 259.6021728515625\n",
            "Epoch [350/800]: Training Loss: 181.3203582763672; Validation Loss: 259.4114685058594\n",
            "Epoch [351/800]: Training Loss: 180.3677215576172; Validation Loss: 259.240478515625\n",
            "Epoch [352/800]: Training Loss: 179.43701171875; Validation Loss: 259.0885314941406\n",
            "Epoch [353/800]: Training Loss: 178.52767944335938; Validation Loss: 258.9549255371094\n",
            "Epoch [354/800]: Training Loss: 177.6392364501953; Validation Loss: 258.8390197753906\n",
            "Epoch [355/800]: Training Loss: 176.77102661132812; Validation Loss: 258.74017333984375\n",
            "Epoch [356/800]: Training Loss: 175.9226531982422; Validation Loss: 258.6576843261719\n",
            "Epoch [357/800]: Training Loss: 175.09356689453125; Validation Loss: 258.5910339355469\n",
            "Epoch [358/800]: Training Loss: 174.28334045410156; Validation Loss: 258.53961181640625\n",
            "Epoch [359/800]: Training Loss: 173.49142456054688; Validation Loss: 258.50286865234375\n",
            "Epoch [360/800]: Training Loss: 172.71739196777344; Validation Loss: 258.4802551269531\n",
            "Epoch [361/800]: Training Loss: 171.96083068847656; Validation Loss: 258.47119140625\n",
            "Epoch [362/800]: Training Loss: 171.2212371826172; Validation Loss: 258.4751281738281\n",
            "Epoch [363/800]: Training Loss: 170.4982452392578; Validation Loss: 258.49163818359375\n",
            "Epoch [364/800]: Training Loss: 169.7914276123047; Validation Loss: 258.520263671875\n",
            "Epoch [365/800]: Training Loss: 169.10037231445312; Validation Loss: 258.5603942871094\n",
            "Epoch [366/800]: Training Loss: 168.4246826171875; Validation Loss: 258.6116638183594\n",
            "Epoch [367/800]: Training Loss: 167.76402282714844; Validation Loss: 258.673583984375\n",
            "Epoch [368/800]: Training Loss: 167.11793518066406; Validation Loss: 258.7457580566406\n",
            "Epoch [369/800]: Training Loss: 166.48614501953125; Validation Loss: 258.8277893066406\n",
            "Epoch [370/800]: Training Loss: 165.8682403564453; Validation Loss: 258.9191589355469\n",
            "Epoch [371/800]: Training Loss: 165.26393127441406; Validation Loss: 259.0195617675781\n",
            "Epoch [372/800]: Training Loss: 164.67283630371094; Validation Loss: 259.1285705566406\n",
            "Epoch [373/800]: Training Loss: 164.0946807861328; Validation Loss: 259.245849609375\n",
            "Epoch [374/800]: Training Loss: 163.52911376953125; Validation Loss: 259.3710021972656\n",
            "Epoch [375/800]: Training Loss: 162.975830078125; Validation Loss: 259.50372314453125\n",
            "Epoch [376/800]: Training Loss: 162.43455505371094; Validation Loss: 259.6435852050781\n",
            "Epoch [377/800]: Training Loss: 161.90496826171875; Validation Loss: 259.79034423828125\n",
            "Epoch [378/800]: Training Loss: 161.38681030273438; Validation Loss: 259.9436950683594\n",
            "Epoch [379/800]: Training Loss: 160.8797607421875; Validation Loss: 260.1033020019531\n",
            "Epoch [380/800]: Training Loss: 160.38360595703125; Validation Loss: 260.2688903808594\n",
            "Epoch [381/800]: Training Loss: 159.89808654785156; Validation Loss: 260.4400634765625\n",
            "Epoch [382/800]: Training Loss: 159.42288208007812; Validation Loss: 260.6166687011719\n",
            "Epoch [383/800]: Training Loss: 158.9578094482422; Validation Loss: 260.79840087890625\n",
            "Epoch [384/800]: Training Loss: 158.5026092529297; Validation Loss: 260.9850158691406\n",
            "Epoch [385/800]: Training Loss: 158.0570068359375; Validation Loss: 261.17620849609375\n",
            "Epoch [386/800]: Training Loss: 157.62083435058594; Validation Loss: 261.37176513671875\n",
            "Epoch [387/800]: Training Loss: 157.19384765625; Validation Loss: 261.57147216796875\n",
            "Epoch [388/800]: Training Loss: 156.7758026123047; Validation Loss: 261.77508544921875\n",
            "Epoch [389/800]: Training Loss: 156.3665313720703; Validation Loss: 261.9823913574219\n",
            "Epoch [390/800]: Training Loss: 155.96575927734375; Validation Loss: 262.19317626953125\n",
            "Epoch [391/800]: Training Loss: 155.5733642578125; Validation Loss: 262.4072265625\n",
            "Epoch [392/800]: Training Loss: 155.18910217285156; Validation Loss: 262.6243896484375\n",
            "Epoch [393/800]: Training Loss: 154.81280517578125; Validation Loss: 262.84442138671875\n",
            "Epoch [394/800]: Training Loss: 154.44427490234375; Validation Loss: 263.067138671875\n",
            "Epoch [395/800]: Training Loss: 154.08334350585938; Validation Loss: 263.2923583984375\n",
            "Epoch [396/800]: Training Loss: 153.72979736328125; Validation Loss: 263.51995849609375\n",
            "Epoch [397/800]: Training Loss: 153.38351440429688; Validation Loss: 263.749755859375\n",
            "Epoch [398/800]: Training Loss: 153.04428100585938; Validation Loss: 263.9815979003906\n",
            "Epoch [399/800]: Training Loss: 152.7119598388672; Validation Loss: 264.2152404785156\n",
            "Epoch [400/800]: Training Loss: 152.3863983154297; Validation Loss: 264.4507141113281\n",
            "Epoch [401/800]: Training Loss: 152.06741333007812; Validation Loss: 264.687744140625\n",
            "Epoch [402/800]: Training Loss: 151.75486755371094; Validation Loss: 264.9261779785156\n",
            "Epoch [403/800]: Training Loss: 151.4486083984375; Validation Loss: 265.16595458984375\n",
            "Epoch [404/800]: Training Loss: 151.14854431152344; Validation Loss: 265.40692138671875\n",
            "Epoch [405/800]: Training Loss: 150.85446166992188; Validation Loss: 265.6489562988281\n",
            "Epoch [406/800]: Training Loss: 150.56622314453125; Validation Loss: 265.8919372558594\n",
            "Epoch [407/800]: Training Loss: 150.2837677001953; Validation Loss: 266.1357727050781\n",
            "Epoch [408/800]: Training Loss: 150.0068817138672; Validation Loss: 266.38031005859375\n",
            "Epoch [409/800]: Training Loss: 149.7354736328125; Validation Loss: 266.6254577636719\n",
            "Epoch [410/800]: Training Loss: 149.46946716308594; Validation Loss: 266.8711853027344\n",
            "Epoch [411/800]: Training Loss: 149.20867919921875; Validation Loss: 267.1172790527344\n",
            "Epoch [412/800]: Training Loss: 148.95298767089844; Validation Loss: 267.36370849609375\n",
            "Epoch [413/800]: Training Loss: 148.7023162841797; Validation Loss: 267.6103820800781\n",
            "Epoch [414/800]: Training Loss: 148.45652770996094; Validation Loss: 267.8572082519531\n",
            "Epoch [415/800]: Training Loss: 148.2155303955078; Validation Loss: 268.10406494140625\n",
            "Epoch [416/800]: Training Loss: 147.97918701171875; Validation Loss: 268.3509826660156\n",
            "Epoch [417/800]: Training Loss: 147.74745178222656; Validation Loss: 268.5977478027344\n",
            "Epoch [418/800]: Training Loss: 147.52015686035156; Validation Loss: 268.8443908691406\n",
            "Epoch [419/800]: Training Loss: 147.2972412109375; Validation Loss: 269.09075927734375\n",
            "Epoch [420/800]: Training Loss: 147.07859802246094; Validation Loss: 269.33685302734375\n",
            "Epoch [421/800]: Training Loss: 146.8641357421875; Validation Loss: 269.5826110839844\n",
            "Epoch [422/800]: Training Loss: 146.65374755859375; Validation Loss: 269.8279113769531\n",
            "Epoch [423/800]: Training Loss: 146.44735717773438; Validation Loss: 270.0727233886719\n",
            "Epoch [424/800]: Training Loss: 146.244873046875; Validation Loss: 270.3170471191406\n",
            "Epoch [425/800]: Training Loss: 146.04623413085938; Validation Loss: 270.5606689453125\n",
            "Epoch [426/800]: Training Loss: 145.85134887695312; Validation Loss: 270.8037109375\n",
            "Epoch [427/800]: Training Loss: 145.66009521484375; Validation Loss: 271.0460510253906\n",
            "Epoch [428/800]: Training Loss: 145.472412109375; Validation Loss: 271.2876281738281\n",
            "Epoch [429/800]: Training Loss: 145.28823852539062; Validation Loss: 271.5284423828125\n",
            "Epoch [430/800]: Training Loss: 145.1074981689453; Validation Loss: 271.7684020996094\n",
            "Epoch [431/800]: Training Loss: 144.93008422851562; Validation Loss: 272.0074768066406\n",
            "Epoch [432/800]: Training Loss: 144.75595092773438; Validation Loss: 272.2456359863281\n",
            "Epoch [433/800]: Training Loss: 144.58505249023438; Validation Loss: 272.4828186035156\n",
            "Epoch [434/800]: Training Loss: 144.41725158691406; Validation Loss: 272.71905517578125\n",
            "Epoch [435/800]: Training Loss: 144.25253295898438; Validation Loss: 272.9542541503906\n",
            "Epoch [436/800]: Training Loss: 144.09083557128906; Validation Loss: 273.18841552734375\n",
            "Epoch [437/800]: Training Loss: 143.93203735351562; Validation Loss: 273.4214782714844\n",
            "Epoch [438/800]: Training Loss: 143.77613830566406; Validation Loss: 273.6534423828125\n",
            "Epoch [439/800]: Training Loss: 143.62307739257812; Validation Loss: 273.8842468261719\n",
            "Epoch [440/800]: Training Loss: 143.47274780273438; Validation Loss: 274.1138916015625\n",
            "Epoch [441/800]: Training Loss: 143.32510375976562; Validation Loss: 274.34234619140625\n",
            "Epoch [442/800]: Training Loss: 143.18011474609375; Validation Loss: 274.56951904296875\n",
            "Epoch [443/800]: Training Loss: 143.03770446777344; Validation Loss: 274.7955627441406\n",
            "Epoch [444/800]: Training Loss: 142.89781188964844; Validation Loss: 275.0202941894531\n",
            "Epoch [445/800]: Training Loss: 142.76040649414062; Validation Loss: 275.2437438964844\n",
            "Epoch [446/800]: Training Loss: 142.62539672851562; Validation Loss: 275.46588134765625\n",
            "Epoch [447/800]: Training Loss: 142.49276733398438; Validation Loss: 275.68670654296875\n",
            "Epoch [448/800]: Training Loss: 142.3624725341797; Validation Loss: 275.90618896484375\n",
            "Epoch [449/800]: Training Loss: 142.2344512939453; Validation Loss: 276.12432861328125\n",
            "Epoch [450/800]: Training Loss: 142.10862731933594; Validation Loss: 276.3410949707031\n",
            "Epoch [451/800]: Training Loss: 141.9850311279297; Validation Loss: 276.5565490722656\n",
            "Epoch [452/800]: Training Loss: 141.86351013183594; Validation Loss: 276.7705383300781\n",
            "Epoch [453/800]: Training Loss: 141.74411010742188; Validation Loss: 276.983154296875\n",
            "Epoch [454/800]: Training Loss: 141.62673950195312; Validation Loss: 277.19439697265625\n",
            "Epoch [455/800]: Training Loss: 141.51136779785156; Validation Loss: 277.4041748046875\n",
            "Epoch [456/800]: Training Loss: 141.39794921875; Validation Loss: 277.6125793457031\n",
            "Epoch [457/800]: Training Loss: 141.28646850585938; Validation Loss: 277.8194580078125\n",
            "Epoch [458/800]: Training Loss: 141.17684936523438; Validation Loss: 278.0249328613281\n",
            "Epoch [459/800]: Training Loss: 141.069091796875; Validation Loss: 278.2289733886719\n",
            "Epoch [460/800]: Training Loss: 140.96311950683594; Validation Loss: 278.4315185546875\n",
            "Epoch [461/800]: Training Loss: 140.85891723632812; Validation Loss: 278.6326599121094\n",
            "Epoch [462/800]: Training Loss: 140.75645446777344; Validation Loss: 278.8323669433594\n",
            "Epoch [463/800]: Training Loss: 140.65565490722656; Validation Loss: 279.0304870605469\n",
            "Epoch [464/800]: Training Loss: 140.55653381347656; Validation Loss: 279.22723388671875\n",
            "Epoch [465/800]: Training Loss: 140.45904541015625; Validation Loss: 279.4224853515625\n",
            "Epoch [466/800]: Training Loss: 140.36312866210938; Validation Loss: 279.6162109375\n",
            "Epoch [467/800]: Training Loss: 140.26876831054688; Validation Loss: 279.8084716796875\n",
            "Epoch [468/800]: Training Loss: 140.17596435546875; Validation Loss: 279.999267578125\n",
            "Epoch [469/800]: Training Loss: 140.08462524414062; Validation Loss: 280.1885681152344\n",
            "Epoch [470/800]: Training Loss: 139.9947509765625; Validation Loss: 280.3763732910156\n",
            "Epoch [471/800]: Training Loss: 139.9063262939453; Validation Loss: 280.5627136230469\n",
            "Epoch [472/800]: Training Loss: 139.8192901611328; Validation Loss: 280.7475891113281\n",
            "Epoch [473/800]: Training Loss: 139.73365783691406; Validation Loss: 280.9309387207031\n",
            "Epoch [474/800]: Training Loss: 139.64935302734375; Validation Loss: 281.1128234863281\n",
            "Epoch [475/800]: Training Loss: 139.56639099121094; Validation Loss: 281.293212890625\n",
            "Epoch [476/800]: Training Loss: 139.48471069335938; Validation Loss: 281.47210693359375\n",
            "Epoch [477/800]: Training Loss: 139.404296875; Validation Loss: 281.6495666503906\n",
            "Epoch [478/800]: Training Loss: 139.32513427734375; Validation Loss: 281.8255920410156\n",
            "Epoch [479/800]: Training Loss: 139.2471923828125; Validation Loss: 282.00006103515625\n",
            "Epoch [480/800]: Training Loss: 139.17044067382812; Validation Loss: 282.1731262207031\n",
            "Epoch [481/800]: Training Loss: 139.09486389160156; Validation Loss: 282.3446960449219\n",
            "Epoch [482/800]: Training Loss: 139.0204315185547; Validation Loss: 282.5148620605469\n",
            "Epoch [483/800]: Training Loss: 138.94712829589844; Validation Loss: 282.68353271484375\n",
            "Epoch [484/800]: Training Loss: 138.87490844726562; Validation Loss: 282.85076904296875\n",
            "Epoch [485/800]: Training Loss: 138.8037872314453; Validation Loss: 283.01654052734375\n",
            "Epoch [486/800]: Training Loss: 138.7337188720703; Validation Loss: 283.180908203125\n",
            "Epoch [487/800]: Training Loss: 138.66468811035156; Validation Loss: 283.3438415527344\n",
            "Epoch [488/800]: Training Loss: 138.59669494628906; Validation Loss: 283.50531005859375\n",
            "Epoch [489/800]: Training Loss: 138.52967834472656; Validation Loss: 283.6653747558594\n",
            "Epoch [490/800]: Training Loss: 138.46363830566406; Validation Loss: 283.82403564453125\n",
            "Epoch [491/800]: Training Loss: 138.3985595703125; Validation Loss: 283.9812927246094\n",
            "Epoch [492/800]: Training Loss: 138.33441162109375; Validation Loss: 284.13714599609375\n",
            "Epoch [493/800]: Training Loss: 138.27120971679688; Validation Loss: 284.2915344238281\n",
            "Epoch [494/800]: Training Loss: 138.20889282226562; Validation Loss: 284.4446105957031\n",
            "Epoch [495/800]: Training Loss: 138.14747619628906; Validation Loss: 284.5962829589844\n",
            "Epoch [496/800]: Training Loss: 138.08689880371094; Validation Loss: 284.7466125488281\n",
            "Epoch [497/800]: Training Loss: 138.02719116210938; Validation Loss: 284.89544677734375\n",
            "Epoch [498/800]: Training Loss: 137.96832275390625; Validation Loss: 285.0429992675781\n",
            "Epoch [499/800]: Training Loss: 137.91026306152344; Validation Loss: 285.1891784667969\n",
            "Epoch [500/800]: Training Loss: 137.85302734375; Validation Loss: 285.33404541015625\n",
            "Epoch [501/800]: Training Loss: 137.79653930664062; Validation Loss: 285.4775390625\n",
            "Epoch [502/800]: Training Loss: 137.74082946777344; Validation Loss: 285.6196594238281\n",
            "Epoch [503/800]: Training Loss: 137.68588256835938; Validation Loss: 285.7605285644531\n",
            "Epoch [504/800]: Training Loss: 137.6317138671875; Validation Loss: 285.9000549316406\n",
            "Epoch [505/800]: Training Loss: 137.57821655273438; Validation Loss: 286.03826904296875\n",
            "Epoch [506/800]: Training Loss: 137.5254669189453; Validation Loss: 286.1752014160156\n",
            "Epoch [507/800]: Training Loss: 137.47340393066406; Validation Loss: 286.3108215332031\n",
            "Epoch [508/800]: Training Loss: 137.4220428466797; Validation Loss: 286.44512939453125\n",
            "Epoch [509/800]: Training Loss: 137.371337890625; Validation Loss: 286.57818603515625\n",
            "Epoch [510/800]: Training Loss: 137.32130432128906; Validation Loss: 286.7099914550781\n",
            "Epoch [511/800]: Training Loss: 137.27191162109375; Validation Loss: 286.8404541015625\n",
            "Epoch [512/800]: Training Loss: 137.22315979003906; Validation Loss: 286.9697570800781\n",
            "Epoch [513/800]: Training Loss: 137.17501831054688; Validation Loss: 287.0977783203125\n",
            "Epoch [514/800]: Training Loss: 137.12750244140625; Validation Loss: 287.2245178222656\n",
            "Epoch [515/800]: Training Loss: 137.08058166503906; Validation Loss: 287.35003662109375\n",
            "Epoch [516/800]: Training Loss: 137.0342559814453; Validation Loss: 287.47442626953125\n",
            "Epoch [517/800]: Training Loss: 136.98851013183594; Validation Loss: 287.5975341796875\n",
            "Epoch [518/800]: Training Loss: 136.94329833984375; Validation Loss: 287.71942138671875\n",
            "Epoch [519/800]: Training Loss: 136.89866638183594; Validation Loss: 287.84014892578125\n",
            "Epoch [520/800]: Training Loss: 136.85458374023438; Validation Loss: 287.959716796875\n",
            "Epoch [521/800]: Training Loss: 136.81101989746094; Validation Loss: 288.0780334472656\n",
            "Epoch [522/800]: Training Loss: 136.7679901123047; Validation Loss: 288.1952209472656\n",
            "Epoch [523/800]: Training Loss: 136.7254638671875; Validation Loss: 288.31121826171875\n",
            "Epoch [524/800]: Training Loss: 136.68345642089844; Validation Loss: 288.4260559082031\n",
            "Epoch [525/800]: Training Loss: 136.64195251464844; Validation Loss: 288.5398254394531\n",
            "Epoch [526/800]: Training Loss: 136.60092163085938; Validation Loss: 288.6524353027344\n",
            "Epoch [527/800]: Training Loss: 136.56036376953125; Validation Loss: 288.763916015625\n",
            "Epoch [528/800]: Training Loss: 136.52029418945312; Validation Loss: 288.874267578125\n",
            "Epoch [529/800]: Training Loss: 136.48065185546875; Validation Loss: 288.9835205078125\n",
            "Epoch [530/800]: Training Loss: 136.4414825439453; Validation Loss: 289.0916748046875\n",
            "Epoch [531/800]: Training Loss: 136.4027557373047; Validation Loss: 289.1986999511719\n",
            "Epoch [532/800]: Training Loss: 136.3644561767578; Validation Loss: 289.3046875\n",
            "Epoch [533/800]: Training Loss: 136.3265838623047; Validation Loss: 289.4096374511719\n",
            "Epoch [534/800]: Training Loss: 136.28912353515625; Validation Loss: 289.513427734375\n",
            "Epoch [535/800]: Training Loss: 136.25210571289062; Validation Loss: 289.6162414550781\n",
            "Epoch [536/800]: Training Loss: 136.2154541015625; Validation Loss: 289.7179260253906\n",
            "Epoch [537/800]: Training Loss: 136.17921447753906; Validation Loss: 289.8186340332031\n",
            "Epoch [538/800]: Training Loss: 136.1433563232422; Validation Loss: 289.9182434082031\n",
            "Epoch [539/800]: Training Loss: 136.10789489746094; Validation Loss: 290.01690673828125\n",
            "Epoch [540/800]: Training Loss: 136.0727996826172; Validation Loss: 290.114501953125\n",
            "Epoch [541/800]: Training Loss: 136.03805541992188; Validation Loss: 290.21112060546875\n",
            "Epoch [542/800]: Training Loss: 136.00369262695312; Validation Loss: 290.3067321777344\n",
            "Epoch [543/800]: Training Loss: 135.96966552734375; Validation Loss: 290.4013671875\n",
            "Epoch [544/800]: Training Loss: 135.93600463867188; Validation Loss: 290.4950256347656\n",
            "Epoch [545/800]: Training Loss: 135.90267944335938; Validation Loss: 290.5876770019531\n",
            "Epoch [546/800]: Training Loss: 135.8696746826172; Validation Loss: 290.6794128417969\n",
            "Epoch [547/800]: Training Loss: 135.83702087402344; Validation Loss: 290.7701721191406\n",
            "Epoch [548/800]: Training Loss: 135.8046875; Validation Loss: 290.8599853515625\n",
            "Epoch [549/800]: Training Loss: 135.77264404296875; Validation Loss: 290.9488525390625\n",
            "Epoch [550/800]: Training Loss: 135.7409210205078; Validation Loss: 291.03680419921875\n",
            "Epoch [551/800]: Training Loss: 135.7095184326172; Validation Loss: 291.1238098144531\n",
            "Epoch [552/800]: Training Loss: 135.67840576171875; Validation Loss: 291.2099304199219\n",
            "Epoch [553/800]: Training Loss: 135.64759826660156; Validation Loss: 291.2951354980469\n",
            "Epoch [554/800]: Training Loss: 135.6170654296875; Validation Loss: 291.3794250488281\n",
            "Epoch [555/800]: Training Loss: 135.5868377685547; Validation Loss: 291.4627990722656\n",
            "Epoch [556/800]: Training Loss: 135.55685424804688; Validation Loss: 291.54534912109375\n",
            "Epoch [557/800]: Training Loss: 135.5271759033203; Validation Loss: 291.6269836425781\n",
            "Epoch [558/800]: Training Loss: 135.4977569580078; Validation Loss: 291.70782470703125\n",
            "Epoch [559/800]: Training Loss: 135.46859741210938; Validation Loss: 291.78778076171875\n",
            "Epoch [560/800]: Training Loss: 135.439697265625; Validation Loss: 291.8668518066406\n",
            "Epoch [561/800]: Training Loss: 135.4110565185547; Validation Loss: 291.9450988769531\n",
            "Epoch [562/800]: Training Loss: 135.38265991210938; Validation Loss: 292.02252197265625\n",
            "Epoch [563/800]: Training Loss: 135.35452270507812; Validation Loss: 292.0990905761719\n",
            "Epoch [564/800]: Training Loss: 135.32659912109375; Validation Loss: 292.1748352050781\n",
            "Epoch [565/800]: Training Loss: 135.2989501953125; Validation Loss: 292.249755859375\n",
            "Epoch [566/800]: Training Loss: 135.27151489257812; Validation Loss: 292.3238830566406\n",
            "Epoch [567/800]: Training Loss: 135.24427795410156; Validation Loss: 292.3972473144531\n",
            "Epoch [568/800]: Training Loss: 135.21731567382812; Validation Loss: 292.4698181152344\n",
            "Epoch [569/800]: Training Loss: 135.1905517578125; Validation Loss: 292.5415954589844\n",
            "Epoch [570/800]: Training Loss: 135.16400146484375; Validation Loss: 292.6125793457031\n",
            "Epoch [571/800]: Training Loss: 135.13766479492188; Validation Loss: 292.68280029296875\n",
            "Epoch [572/800]: Training Loss: 135.1115264892578; Validation Loss: 292.7522888183594\n",
            "Epoch [573/800]: Training Loss: 135.0856170654297; Validation Loss: 292.82098388671875\n",
            "Epoch [574/800]: Training Loss: 135.05990600585938; Validation Loss: 292.8889465332031\n",
            "Epoch [575/800]: Training Loss: 135.0343780517578; Validation Loss: 292.9562072753906\n",
            "Epoch [576/800]: Training Loss: 135.00904846191406; Validation Loss: 293.0227355957031\n",
            "Epoch [577/800]: Training Loss: 134.98391723632812; Validation Loss: 293.0885009765625\n",
            "Epoch [578/800]: Training Loss: 134.95896911621094; Validation Loss: 293.153564453125\n",
            "Epoch [579/800]: Training Loss: 134.9342041015625; Validation Loss: 293.2179260253906\n",
            "Epoch [580/800]: Training Loss: 134.90960693359375; Validation Loss: 293.28155517578125\n",
            "Epoch [581/800]: Training Loss: 134.88522338867188; Validation Loss: 293.3445129394531\n",
            "Epoch [582/800]: Training Loss: 134.86099243164062; Validation Loss: 293.40673828125\n",
            "Epoch [583/800]: Training Loss: 134.83692932128906; Validation Loss: 293.4683532714844\n",
            "Epoch [584/800]: Training Loss: 134.8130340576172; Validation Loss: 293.529296875\n",
            "Epoch [585/800]: Training Loss: 134.78932189941406; Validation Loss: 293.5894775390625\n",
            "Epoch [586/800]: Training Loss: 134.76573181152344; Validation Loss: 293.64910888671875\n",
            "Epoch [587/800]: Training Loss: 134.74234008789062; Validation Loss: 293.7079772949219\n",
            "Epoch [588/800]: Training Loss: 134.71908569335938; Validation Loss: 293.7662353515625\n",
            "Epoch [589/800]: Training Loss: 134.6959991455078; Validation Loss: 293.82391357421875\n",
            "Epoch [590/800]: Training Loss: 134.6730499267578; Validation Loss: 293.8808898925781\n",
            "Epoch [591/800]: Training Loss: 134.65025329589844; Validation Loss: 293.937255859375\n",
            "Epoch [592/800]: Training Loss: 134.6276092529297; Validation Loss: 293.992919921875\n",
            "Epoch [593/800]: Training Loss: 134.6051025390625; Validation Loss: 294.048095703125\n",
            "Epoch [594/800]: Training Loss: 134.58273315429688; Validation Loss: 294.1025695800781\n",
            "Epoch [595/800]: Training Loss: 134.56051635742188; Validation Loss: 294.1564636230469\n",
            "Epoch [596/800]: Training Loss: 134.53843688964844; Validation Loss: 294.2097473144531\n",
            "Epoch [597/800]: Training Loss: 134.5164794921875; Validation Loss: 294.262451171875\n",
            "Epoch [598/800]: Training Loss: 134.49465942382812; Validation Loss: 294.3145751953125\n",
            "Epoch [599/800]: Training Loss: 134.47296142578125; Validation Loss: 294.36614990234375\n",
            "Epoch [600/800]: Training Loss: 134.45140075683594; Validation Loss: 294.4170837402344\n",
            "Epoch [601/800]: Training Loss: 134.42996215820312; Validation Loss: 294.46746826171875\n",
            "Epoch [602/800]: Training Loss: 134.40863037109375; Validation Loss: 294.5173034667969\n",
            "Epoch [603/800]: Training Loss: 134.38743591308594; Validation Loss: 294.56658935546875\n",
            "Epoch [604/800]: Training Loss: 134.36634826660156; Validation Loss: 294.61529541015625\n",
            "Epoch [605/800]: Training Loss: 134.34536743164062; Validation Loss: 294.6634826660156\n",
            "Epoch [606/800]: Training Loss: 134.3245391845703; Validation Loss: 294.71112060546875\n",
            "Epoch [607/800]: Training Loss: 134.30380249023438; Validation Loss: 294.7582092285156\n",
            "Epoch [608/800]: Training Loss: 134.2831573486328; Validation Loss: 294.8047790527344\n",
            "Epoch [609/800]: Training Loss: 134.2626190185547; Validation Loss: 294.850830078125\n",
            "Epoch [610/800]: Training Loss: 134.24221801757812; Validation Loss: 294.89630126953125\n",
            "Epoch [611/800]: Training Loss: 134.22190856933594; Validation Loss: 294.94134521484375\n",
            "Epoch [612/800]: Training Loss: 134.20169067382812; Validation Loss: 294.98583984375\n",
            "Epoch [613/800]: Training Loss: 134.18157958984375; Validation Loss: 295.0298156738281\n",
            "Epoch [614/800]: Training Loss: 134.16156005859375; Validation Loss: 295.0733642578125\n",
            "Epoch [615/800]: Training Loss: 134.14163208007812; Validation Loss: 295.1163330078125\n",
            "Epoch [616/800]: Training Loss: 134.12181091308594; Validation Loss: 295.1589050292969\n",
            "Epoch [617/800]: Training Loss: 134.1020965576172; Validation Loss: 295.2009582519531\n",
            "Epoch [618/800]: Training Loss: 134.08245849609375; Validation Loss: 295.24249267578125\n",
            "Epoch [619/800]: Training Loss: 134.0629119873047; Validation Loss: 295.2835388183594\n",
            "Epoch [620/800]: Training Loss: 134.04347229003906; Validation Loss: 295.32421875\n",
            "Epoch [621/800]: Training Loss: 134.0240936279297; Validation Loss: 295.3643493652344\n",
            "Epoch [622/800]: Training Loss: 134.00482177734375; Validation Loss: 295.4040832519531\n",
            "Epoch [623/800]: Training Loss: 133.98562622070312; Validation Loss: 295.443359375\n",
            "Epoch [624/800]: Training Loss: 133.9665069580078; Validation Loss: 295.4821472167969\n",
            "Epoch [625/800]: Training Loss: 133.9474639892578; Validation Loss: 295.5205078125\n",
            "Epoch [626/800]: Training Loss: 133.9285125732422; Validation Loss: 295.55841064453125\n",
            "Epoch [627/800]: Training Loss: 133.9096221923828; Validation Loss: 295.5959167480469\n",
            "Epoch [628/800]: Training Loss: 133.8908233642578; Validation Loss: 295.6330261230469\n",
            "Epoch [629/800]: Training Loss: 133.87210083007812; Validation Loss: 295.6696472167969\n",
            "Epoch [630/800]: Training Loss: 133.85345458984375; Validation Loss: 295.70587158203125\n",
            "Epoch [631/800]: Training Loss: 133.83486938476562; Validation Loss: 295.7416687011719\n",
            "Epoch [632/800]: Training Loss: 133.81634521484375; Validation Loss: 295.7770690917969\n",
            "Epoch [633/800]: Training Loss: 133.7979278564453; Validation Loss: 295.8120422363281\n",
            "Epoch [634/800]: Training Loss: 133.779541015625; Validation Loss: 295.8466796875\n",
            "Epoch [635/800]: Training Loss: 133.76124572753906; Validation Loss: 295.880859375\n",
            "Epoch [636/800]: Training Loss: 133.74301147460938; Validation Loss: 295.9146423339844\n",
            "Epoch [637/800]: Training Loss: 133.72482299804688; Validation Loss: 295.9480895996094\n",
            "Epoch [638/800]: Training Loss: 133.7067413330078; Validation Loss: 295.98114013671875\n",
            "Epoch [639/800]: Training Loss: 133.68869018554688; Validation Loss: 296.0137634277344\n",
            "Epoch [640/800]: Training Loss: 133.6707000732422; Validation Loss: 296.04608154296875\n",
            "Epoch [641/800]: Training Loss: 133.6527862548828; Validation Loss: 296.0779724121094\n",
            "Epoch [642/800]: Training Loss: 133.63490295410156; Validation Loss: 296.1094970703125\n",
            "Epoch [643/800]: Training Loss: 133.6171112060547; Validation Loss: 296.1406555175781\n",
            "Epoch [644/800]: Training Loss: 133.59934997558594; Validation Loss: 296.1714782714844\n",
            "Epoch [645/800]: Training Loss: 133.5816650390625; Validation Loss: 296.2019348144531\n",
            "Epoch [646/800]: Training Loss: 133.5640411376953; Validation Loss: 296.2320861816406\n",
            "Epoch [647/800]: Training Loss: 133.54644775390625; Validation Loss: 296.2618103027344\n",
            "Epoch [648/800]: Training Loss: 133.52891540527344; Validation Loss: 296.291259765625\n",
            "Epoch [649/800]: Training Loss: 133.5114288330078; Validation Loss: 296.3203430175781\n",
            "Epoch [650/800]: Training Loss: 133.49400329589844; Validation Loss: 296.3490905761719\n",
            "Epoch [651/800]: Training Loss: 133.47662353515625; Validation Loss: 296.3774719238281\n",
            "Epoch [652/800]: Training Loss: 133.45928955078125; Validation Loss: 296.405517578125\n",
            "Epoch [653/800]: Training Loss: 133.4420166015625; Validation Loss: 296.43328857421875\n",
            "Epoch [654/800]: Training Loss: 133.42478942871094; Validation Loss: 296.46075439453125\n",
            "Epoch [655/800]: Training Loss: 133.40757751464844; Validation Loss: 296.4878845214844\n",
            "Epoch [656/800]: Training Loss: 133.39044189453125; Validation Loss: 296.5146484375\n",
            "Epoch [657/800]: Training Loss: 133.37335205078125; Validation Loss: 296.5411682128906\n",
            "Epoch [658/800]: Training Loss: 133.3562774658203; Validation Loss: 296.5673522949219\n",
            "Epoch [659/800]: Training Loss: 133.33926391601562; Validation Loss: 296.5932312011719\n",
            "Epoch [660/800]: Training Loss: 133.32228088378906; Validation Loss: 296.6188049316406\n",
            "Epoch [661/800]: Training Loss: 133.3053741455078; Validation Loss: 296.64410400390625\n",
            "Epoch [662/800]: Training Loss: 133.28846740722656; Validation Loss: 296.6690979003906\n",
            "Epoch [663/800]: Training Loss: 133.2716064453125; Validation Loss: 296.6938171386719\n",
            "Epoch [664/800]: Training Loss: 133.25482177734375; Validation Loss: 296.71826171875\n",
            "Epoch [665/800]: Training Loss: 133.238037109375; Validation Loss: 296.74237060546875\n",
            "Epoch [666/800]: Training Loss: 133.22128295898438; Validation Loss: 296.7662048339844\n",
            "Epoch [667/800]: Training Loss: 133.20457458496094; Validation Loss: 296.789794921875\n",
            "Epoch [668/800]: Training Loss: 133.18792724609375; Validation Loss: 296.8131103515625\n",
            "Epoch [669/800]: Training Loss: 133.17127990722656; Validation Loss: 296.8361511230469\n",
            "Epoch [670/800]: Training Loss: 133.15469360351562; Validation Loss: 296.8589172363281\n",
            "Epoch [671/800]: Training Loss: 133.13812255859375; Validation Loss: 296.88140869140625\n",
            "Epoch [672/800]: Training Loss: 133.12158203125; Validation Loss: 296.90362548828125\n",
            "Epoch [673/800]: Training Loss: 133.1051025390625; Validation Loss: 296.92559814453125\n",
            "Epoch [674/800]: Training Loss: 133.08860778808594; Validation Loss: 296.9473571777344\n",
            "Epoch [675/800]: Training Loss: 133.07217407226562; Validation Loss: 296.9688415527344\n",
            "Epoch [676/800]: Training Loss: 133.05575561523438; Validation Loss: 296.9900207519531\n",
            "Epoch [677/800]: Training Loss: 133.03936767578125; Validation Loss: 297.01104736328125\n",
            "Epoch [678/800]: Training Loss: 133.0230255126953; Validation Loss: 297.03173828125\n",
            "Epoch [679/800]: Training Loss: 133.00669860839844; Validation Loss: 297.05224609375\n",
            "Epoch [680/800]: Training Loss: 132.9904022216797; Validation Loss: 297.072509765625\n",
            "Epoch [681/800]: Training Loss: 132.97412109375; Validation Loss: 297.092529296875\n",
            "Epoch [682/800]: Training Loss: 132.9578857421875; Validation Loss: 297.1123352050781\n",
            "Epoch [683/800]: Training Loss: 132.94166564941406; Validation Loss: 297.1318664550781\n",
            "Epoch [684/800]: Training Loss: 132.9254608154297; Validation Loss: 297.15118408203125\n",
            "Epoch [685/800]: Training Loss: 132.9093017578125; Validation Loss: 297.1702880859375\n",
            "Epoch [686/800]: Training Loss: 132.8931427001953; Validation Loss: 297.18914794921875\n",
            "Epoch [687/800]: Training Loss: 132.87701416015625; Validation Loss: 297.20782470703125\n",
            "Epoch [688/800]: Training Loss: 132.8609161376953; Validation Loss: 297.22625732421875\n",
            "Epoch [689/800]: Training Loss: 132.84483337402344; Validation Loss: 297.2444763183594\n",
            "Epoch [690/800]: Training Loss: 132.82876586914062; Validation Loss: 297.26251220703125\n",
            "Epoch [691/800]: Training Loss: 132.81272888183594; Validation Loss: 297.28033447265625\n",
            "Epoch [692/800]: Training Loss: 132.79672241210938; Validation Loss: 297.2979736328125\n",
            "Epoch [693/800]: Training Loss: 132.78070068359375; Validation Loss: 297.3153381347656\n",
            "Epoch [694/800]: Training Loss: 132.76473999023438; Validation Loss: 297.33251953125\n",
            "Epoch [695/800]: Training Loss: 132.74876403808594; Validation Loss: 297.34954833984375\n",
            "Epoch [696/800]: Training Loss: 132.73281860351562; Validation Loss: 297.3663330078125\n",
            "Epoch [697/800]: Training Loss: 132.71688842773438; Validation Loss: 297.3829040527344\n",
            "Epoch [698/800]: Training Loss: 132.70098876953125; Validation Loss: 297.39935302734375\n",
            "Epoch [699/800]: Training Loss: 132.68508911132812; Validation Loss: 297.4156188964844\n",
            "Epoch [700/800]: Training Loss: 132.66920471191406; Validation Loss: 297.4316101074219\n",
            "Epoch [701/800]: Training Loss: 132.65335083007812; Validation Loss: 297.44744873046875\n",
            "Epoch [702/800]: Training Loss: 132.63751220703125; Validation Loss: 297.463134765625\n",
            "Epoch [703/800]: Training Loss: 132.6216583251953; Validation Loss: 297.4786376953125\n",
            "Epoch [704/800]: Training Loss: 132.60585021972656; Validation Loss: 297.4939270019531\n",
            "Epoch [705/800]: Training Loss: 132.5900421142578; Validation Loss: 297.509033203125\n",
            "Epoch [706/800]: Training Loss: 132.57424926757812; Validation Loss: 297.52398681640625\n",
            "Epoch [707/800]: Training Loss: 132.5584716796875; Validation Loss: 297.53875732421875\n",
            "Epoch [708/800]: Training Loss: 132.54270935058594; Validation Loss: 297.55340576171875\n",
            "Epoch [709/800]: Training Loss: 132.52696228027344; Validation Loss: 297.5678405761719\n",
            "Epoch [710/800]: Training Loss: 132.51119995117188; Validation Loss: 297.5821228027344\n",
            "Epoch [711/800]: Training Loss: 132.49546813964844; Validation Loss: 297.59619140625\n",
            "Epoch [712/800]: Training Loss: 132.47976684570312; Validation Loss: 297.6101379394531\n",
            "Epoch [713/800]: Training Loss: 132.46405029296875; Validation Loss: 297.6239318847656\n",
            "Epoch [714/800]: Training Loss: 132.44834899902344; Validation Loss: 297.6375427246094\n",
            "Epoch [715/800]: Training Loss: 132.4326629638672; Validation Loss: 297.6510009765625\n",
            "Epoch [716/800]: Training Loss: 132.41697692871094; Validation Loss: 297.6643371582031\n",
            "Epoch [717/800]: Training Loss: 132.40130615234375; Validation Loss: 297.6775207519531\n",
            "Epoch [718/800]: Training Loss: 132.38563537597656; Validation Loss: 297.69049072265625\n",
            "Epoch [719/800]: Training Loss: 132.36997985839844; Validation Loss: 297.703369140625\n",
            "Epoch [720/800]: Training Loss: 132.35430908203125; Validation Loss: 297.716064453125\n",
            "Epoch [721/800]: Training Loss: 132.33868408203125; Validation Loss: 297.7286376953125\n",
            "Epoch [722/800]: Training Loss: 132.3230438232422; Validation Loss: 297.7410888671875\n",
            "Epoch [723/800]: Training Loss: 132.30743408203125; Validation Loss: 297.7533874511719\n",
            "Epoch [724/800]: Training Loss: 132.29177856445312; Validation Loss: 297.7655334472656\n",
            "Epoch [725/800]: Training Loss: 132.27618408203125; Validation Loss: 297.7775573730469\n",
            "Epoch [726/800]: Training Loss: 132.26055908203125; Validation Loss: 297.7894592285156\n",
            "Epoch [727/800]: Training Loss: 132.2449493408203; Validation Loss: 297.8011779785156\n",
            "Epoch [728/800]: Training Loss: 132.22935485839844; Validation Loss: 297.81280517578125\n",
            "Epoch [729/800]: Training Loss: 132.21376037597656; Validation Loss: 297.8243103027344\n",
            "Epoch [730/800]: Training Loss: 132.1981658935547; Validation Loss: 297.83563232421875\n",
            "Epoch [731/800]: Training Loss: 132.1825714111328; Validation Loss: 297.8468322753906\n",
            "Epoch [732/800]: Training Loss: 132.16697692871094; Validation Loss: 297.8579406738281\n",
            "Epoch [733/800]: Training Loss: 132.1514129638672; Validation Loss: 297.8689270019531\n",
            "Epoch [734/800]: Training Loss: 132.1358184814453; Validation Loss: 297.8797607421875\n",
            "Epoch [735/800]: Training Loss: 132.12025451660156; Validation Loss: 297.8905334472656\n",
            "Epoch [736/800]: Training Loss: 132.1046600341797; Validation Loss: 297.9010925292969\n",
            "Epoch [737/800]: Training Loss: 132.08909606933594; Validation Loss: 297.9115905761719\n",
            "Epoch [738/800]: Training Loss: 132.07351684570312; Validation Loss: 297.9219665527344\n",
            "Epoch [739/800]: Training Loss: 132.0579376220703; Validation Loss: 297.9322204589844\n",
            "Epoch [740/800]: Training Loss: 132.0423583984375; Validation Loss: 297.9423522949219\n",
            "Epoch [741/800]: Training Loss: 132.0268096923828; Validation Loss: 297.952392578125\n",
            "Epoch [742/800]: Training Loss: 132.01123046875; Validation Loss: 297.9623107910156\n",
            "Epoch [743/800]: Training Loss: 131.9956512451172; Validation Loss: 297.9720764160156\n",
            "Epoch [744/800]: Training Loss: 131.98008728027344; Validation Loss: 297.9818115234375\n",
            "Epoch [745/800]: Training Loss: 131.96450805664062; Validation Loss: 297.99139404296875\n",
            "Epoch [746/800]: Training Loss: 131.9489288330078; Validation Loss: 298.0008850097656\n",
            "Epoch [747/800]: Training Loss: 131.93336486816406; Validation Loss: 298.0102844238281\n",
            "Epoch [748/800]: Training Loss: 131.9177703857422; Validation Loss: 298.0195617675781\n",
            "Epoch [749/800]: Training Loss: 131.90220642089844; Validation Loss: 298.0287780761719\n",
            "Epoch [750/800]: Training Loss: 131.88662719726562; Validation Loss: 298.03778076171875\n",
            "Epoch [751/800]: Training Loss: 131.8710479736328; Validation Loss: 298.04681396484375\n",
            "Epoch [752/800]: Training Loss: 131.85545349121094; Validation Loss: 298.0556945800781\n",
            "Epoch [753/800]: Training Loss: 131.83987426757812; Validation Loss: 298.0644836425781\n",
            "Epoch [754/800]: Training Loss: 131.82427978515625; Validation Loss: 298.0731506347656\n",
            "Epoch [755/800]: Training Loss: 131.80868530273438; Validation Loss: 298.081787109375\n",
            "Epoch [756/800]: Training Loss: 131.7930908203125; Validation Loss: 298.09027099609375\n",
            "Epoch [757/800]: Training Loss: 131.77748107910156; Validation Loss: 298.09869384765625\n",
            "Epoch [758/800]: Training Loss: 131.7618865966797; Validation Loss: 298.10699462890625\n",
            "Epoch [759/800]: Training Loss: 131.74624633789062; Validation Loss: 298.1152038574219\n",
            "Epoch [760/800]: Training Loss: 131.73065185546875; Validation Loss: 298.1233825683594\n",
            "Epoch [761/800]: Training Loss: 131.71502685546875; Validation Loss: 298.1314392089844\n",
            "Epoch [762/800]: Training Loss: 131.69940185546875; Validation Loss: 298.139404296875\n",
            "Epoch [763/800]: Training Loss: 131.6837615966797; Validation Loss: 298.1473388671875\n",
            "Epoch [764/800]: Training Loss: 131.6681365966797; Validation Loss: 298.1551513671875\n",
            "Epoch [765/800]: Training Loss: 131.6524658203125; Validation Loss: 298.16290283203125\n",
            "Epoch [766/800]: Training Loss: 131.6368408203125; Validation Loss: 298.1705627441406\n",
            "Epoch [767/800]: Training Loss: 131.62118530273438; Validation Loss: 298.1781005859375\n",
            "Epoch [768/800]: Training Loss: 131.60549926757812; Validation Loss: 298.1855773925781\n",
            "Epoch [769/800]: Training Loss: 131.58985900878906; Validation Loss: 298.1930236816406\n",
            "Epoch [770/800]: Training Loss: 131.5741729736328; Validation Loss: 298.2003173828125\n",
            "Epoch [771/800]: Training Loss: 131.55850219726562; Validation Loss: 298.20758056640625\n",
            "Epoch [772/800]: Training Loss: 131.5428009033203; Validation Loss: 298.2147521972656\n",
            "Epoch [773/800]: Training Loss: 131.52708435058594; Validation Loss: 298.2218933105469\n",
            "Epoch [774/800]: Training Loss: 131.51138305664062; Validation Loss: 298.22894287109375\n",
            "Epoch [775/800]: Training Loss: 131.4956817626953; Validation Loss: 298.23590087890625\n",
            "Epoch [776/800]: Training Loss: 131.47994995117188; Validation Loss: 298.2427978515625\n",
            "Epoch [777/800]: Training Loss: 131.4642333984375; Validation Loss: 298.2496643066406\n",
            "Epoch [778/800]: Training Loss: 131.448486328125; Validation Loss: 298.2563781738281\n",
            "Epoch [779/800]: Training Loss: 131.43272399902344; Validation Loss: 298.2630920410156\n",
            "Epoch [780/800]: Training Loss: 131.41697692871094; Validation Loss: 298.2697448730469\n",
            "Epoch [781/800]: Training Loss: 131.4011993408203; Validation Loss: 298.2763366699219\n",
            "Epoch [782/800]: Training Loss: 131.38540649414062; Validation Loss: 298.28277587890625\n",
            "Epoch [783/800]: Training Loss: 131.36962890625; Validation Loss: 298.2892150878906\n",
            "Epoch [784/800]: Training Loss: 131.3538360595703; Validation Loss: 298.2955627441406\n",
            "Epoch [785/800]: Training Loss: 131.33802795410156; Validation Loss: 298.3018798828125\n",
            "Epoch [786/800]: Training Loss: 131.32220458984375; Validation Loss: 298.30816650390625\n",
            "Epoch [787/800]: Training Loss: 131.30636596679688; Validation Loss: 298.3143615722656\n",
            "Epoch [788/800]: Training Loss: 131.29052734375; Validation Loss: 298.32049560546875\n",
            "Epoch [789/800]: Training Loss: 131.27468872070312; Validation Loss: 298.3265686035156\n",
            "Epoch [790/800]: Training Loss: 131.25881958007812; Validation Loss: 298.33258056640625\n",
            "Epoch [791/800]: Training Loss: 131.24295043945312; Validation Loss: 298.3385314941406\n",
            "Epoch [792/800]: Training Loss: 131.22705078125; Validation Loss: 298.3444519042969\n",
            "Epoch [793/800]: Training Loss: 131.21116638183594; Validation Loss: 298.3503112792969\n",
            "Epoch [794/800]: Training Loss: 131.19525146484375; Validation Loss: 298.3561096191406\n",
            "Epoch [795/800]: Training Loss: 131.17933654785156; Validation Loss: 298.36181640625\n",
            "Epoch [796/800]: Training Loss: 131.1634063720703; Validation Loss: 298.36749267578125\n",
            "Epoch [797/800]: Training Loss: 131.1474609375; Validation Loss: 298.3731384277344\n",
            "Epoch [798/800]: Training Loss: 131.13148498535156; Validation Loss: 298.37872314453125\n",
            "Epoch [799/800]: Training Loss: 131.11549377441406; Validation Loss: 298.3842468261719\n",
            "Epoch [800/800]: Training Loss: 131.09951782226562; Validation Loss: 298.38970947265625\n",
            "Best Validation Epoch:  361\n",
            "Best Validation Loss:  tensor(258.4712)\n"
          ]
        }
      ]
    }
  ]
}